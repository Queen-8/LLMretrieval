{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c26b4ee",
   "metadata": {},
   "source": [
    "åŠ è½½æ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents: 4\n",
      "First document text: 1\n",
      "CSRF\n",
      "1. åŸç†\n",
      "2. ä¸XSSåŒºåˆ«\n",
      "3. å¸¸è§åœºæ™¯\n",
      "4. å¸¸è§æ¼æ´ç‚¹\n",
      "5. æ¼æ´å±å®³\n",
      "6. CSRF Poc æ„é€ \n",
      "7. æ¼æ´å®¡è®¡\n",
      "8. æ¼æ´ä¿®å¤\n",
      "9. Webgoat\n",
      "è·¨ç«™è¯·æ±‚ä¼ªé€ ï¼ˆCro\n",
      "First document text: 1\n",
      "XXE æ¼æ´\n",
      "1. xmlåŸºæœ¬ä»‹ç»\n",
      "1.1. ä»€ä¹ˆæ˜¯xml\n",
      "1.2. xml å†…å®¹ç¤ºä¾‹\n",
      "1.2.1. DTD çº¦æŸ\n",
      "1.2.2. å†…éƒ¨å®ä½“ Internal Entity\n",
      "1.2.3. å¤–éƒ¨å®ä½“ \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import fitz  # PyMuPDF, ç”¨äºPDFæ–‡æœ¬æå–\n",
    "from dotenv import load_dotenv  # ç”¨äºåŠ è½½.envç¯å¢ƒå˜é‡\n",
    "from langchain.vectorstores import Chroma  # å‘é‡æ•°æ®åº“\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings  # å‘é‡åŒ–\n",
    "from langchain.text_splitter import CharacterTextSplitter  # æ–‡æœ¬åˆ†ç‰‡\n",
    "from langchain.llms import OpenAI  # OpenAI LLMè°ƒç”¨\n",
    "\n",
    "# åŠ è½½æœ¬åœ°.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ˆå¦‚API KEYï¼‰\n",
    "load_dotenv()\n",
    "\n",
    "# è¯»å–OpenAIå¯†é’¥\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# è®¾ç½®ä½¿ç”¨çš„æ¨¡å‹ï¼ˆOpenRouterä¸Šçš„GPT-4o-miniï¼‰\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# é…ç½®æ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯•\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_pdf_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load all the PDF files in the specified folder, \n",
    "    returning the full text content of each file (string list).\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    # Iterate over all pdf files\n",
    "    for filename in glob.glob(f\"Local_knowledge_base/*.pdf\"):\n",
    "        document_text = \"\"\n",
    "        # Open the PDF and iterate over each page, accumulating the text\n",
    "        with fitz.open(filename) as doc:\n",
    "            for page_num in range(doc.page_count):\n",
    "                page = doc.load_page(page_num)\n",
    "                document_text += page.get_text(\"text\")\n",
    "        all_documents.append(document_text)\n",
    "    return all_documents\n",
    "\n",
    "\n",
    "# è°ƒç”¨å‡½æ•°å¹¶æ‰“å°\n",
    "all_documents = load_pdf_from_folder(\"Local_knowledge_base\")  # ä¼ å…¥PDFæ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„\n",
    "print(\"Loaded documents:\", len(all_documents))\n",
    "print(\"First document text:\", all_documents[0][:100])  # æ‰“å°ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å‰100ä¸ªå­—ç¬¦\n",
    "print(\"Second document text:\", all_documents[1][:100])  # æ‰“å°ç¬¬äºŒä¸ªæ–‡æ¡£çš„å‰100ä¸ªå­—ç¬¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294c07a",
   "metadata": {},
   "source": [
    "åˆ†ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f13ca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1976, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1961, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 1465, which is longer than the specified 1000\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 6692, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document chunks: 15\n",
      "First chunk text: 1\n",
      "CSRF\n",
      "1. åŸç†\n",
      "2. ä¸XSSåŒºåˆ«\n",
      "3. å¸¸è§åœºæ™¯\n",
      "4. å¸¸è§æ¼æ´ç‚¹\n",
      "5. æ¼æ´å±å®³\n",
      "6. CSRF Poc æ„é€ \n",
      "7. æ¼æ´å®¡è®¡\n",
      "8. æ¼æ´ä¿®å¤\n",
      "9. Webgoat\n",
      "è·¨ç«™è¯·æ±‚ä¼ªé€ ï¼ˆCro\n",
      "Second chunk text: ä¸€æ¬¡å®Œæ•´çš„ CSRF æ”»å‡»éœ€è¦å…·å¤‡ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ï¼š\n",
      "ç”¨æˆ·å·²ç»ç™»å½•æŸç«™ç‚¹ï¼Œå¹¶ä¸”åœ¨æµè§ˆå™¨ä¸­å­˜å‚¨äº†ç™»å½•åçš„ Cookie ä¿¡æ¯ã€‚\n",
      "åœ¨ä¸æ³¨é”€æŸç«™ç‚¹çš„æƒ…å†µä¸‹ï¼Œå»è®¿é—®æ”»å‡»è€…æ„é€ çš„ç«™ç‚¹ã€‚\n",
      "ä¾‹ï¼š\n",
      "ç½‘ç«™ç®¡ç†å‘˜æ·»åŠ ç”¨æˆ·çš„ \n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, overlap=100):\n",
    "    \"\"\"\n",
    "    å¯¹æ¯ä¸ªæ–‡æ¡£åšåˆ†ç‰‡ï¼Œchunk_sizeæ§åˆ¶å•ç‰‡é•¿åº¦ï¼Œoverlapä¿è¯ä¸Šä¸‹æ–‡è¿è´¯\n",
    "    è¾“å…¥: å­—ç¬¦ä¸²listï¼Œè¾“å‡º: åˆ†ç‰‡list\n",
    "    \"\"\"\n",
    "    # å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢ä¸ºDocumentå¯¹è±¡åˆ—è¡¨\n",
    "    doc_objects = [Document(page_content=doc) for doc in documents]\n",
    "    \n",
    "    # åˆ›å»ºtext_splitterå¯¹è±¡\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    \n",
    "    # åˆ†ç‰‡å¹¶è¿”å›ç»“æœ\n",
    "    return text_splitter.split_documents(doc_objects)\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "split_docs = split_documents(all_documents, chunk_size=1000, overlap=100)\n",
    "print(\"Number of document chunks:\", len(split_docs))\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "print(\"First chunk text:\", split_docs[0].page_content[:100])  # æ‰“å°ç¬¬ä¸€ä¸ªåˆ†ç‰‡çš„å‰100ä¸ªå­—ç¬¦\n",
    "print(\"Second chunk text:\", split_docs[1].page_content[:100])  # æ‰“å°ç¬¬äºŒä¸ªåˆ†ç‰‡çš„å‰100ä¸ªå­—ç¬¦\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbb309",
   "metadata": {},
   "source": [
    "ç´¢å¼•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "319da18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: shibing624/text2vec-base-chinese\n",
      "/Users/queen/Documents/VSCode/llm_retrieval/.venv/lib/python3.13/site-packages/transformers/models/bert/tokenization_bert.py:120: RuntimeWarning: coroutine 'build_index' was never awaited\n",
      "  self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 57 document chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def build_index(documents):\n",
    "    \"\"\"\n",
    "    Convert the sharded documents into vectors and index them\n",
    "    \"\"\"\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"shibing624/text2vec-base-chinese\")\n",
    "    \n",
    "    # å°†æ–‡æ¡£åˆ—è¡¨è½¬æ¢ä¸ºDocumentå¯¹è±¡ï¼ˆç¡®ä¿docæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼‰\n",
    "    doc_objects = [Document(page_content=str(doc)) for doc in documents]\n",
    "    \n",
    "    # ä½¿ç”¨Chromaåˆ›å»ºå‘é‡ç´¢å¼•\n",
    "    index = Chroma.from_documents(doc_objects, embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹ï¼šå‡è®¾split_docsåŒ…å«åˆ†ç‰‡åçš„æ–‡æ¡£åˆ—è¡¨\n",
    "index = build_index(split_docs)\n",
    "\n",
    "# æ‰“å°ç´¢å¼•ä¿¡æ¯ï¼ˆè·å–ç´¢å¼•ä¸­æ–‡æ¡£æ•°é‡ï¼‰\n",
    "print(\"Index built with\", index._collection.count(), \"document chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30172746",
   "metadata": {},
   "source": [
    "å¬å›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "464e4896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 content: page_content='2024 å¹´ä¸–ç•ŒèŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›\n",
      "åˆ¶åº¦æ±‡ç¼–\n",
      "ä¸–ç•ŒèŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›æ‰§è¡Œå§”å‘˜ä¼šï¼ˆç­¹ï¼‰\n",
      "2024 å¹´9 æœˆ\n",
      "ç›®\n",
      "å½•\n",
      "ä¸–ç•ŒèŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›ç®¡ç†è§„å®šä¸åŠæ³•\n",
      "ç»„ç»‡æœºæ„ä¸èŒèƒ½åˆ†å·¥....\n",
      "Document 2 content: page_content='1\n",
      "SSRFæ¼æ´\n",
      "1. SSRFæ¼æ´\n",
      "1.1. åŸç†\n",
      "1.2. æ¼æ´å±å®³\n",
      "1.3. å®¹æ˜“å‡ºç°æ¼æ´çš„åœ°æ–¹\n",
      "2. æ¼æ´å®¡è®¡ç‚¹\n",
      "2.1. URLConnection\n",
      "2.2. H\n",
      "Document 3 content: page_content='1\n",
      "SSRFæ¼æ´\n",
      "1. SSRFæ¼æ´\n",
      "1.1. åŸç†\n",
      "1.2. æ¼æ´å±å®³\n",
      "1.3. å®¹æ˜“å‡ºç°æ¼æ´çš„åœ°æ–¹\n",
      "2. æ¼æ´å®¡è®¡ç‚¹\n",
      "2.1. URLConnection\n",
      "2.2. H\n",
      "Document 4 content: page_content='1\n",
      "SSRFæ¼æ´\n",
      "1. SSRFæ¼æ´\n",
      "1.1. åŸç†\n",
      "1.2. æ¼æ´å±å®³\n",
      "1.3. å®¹æ˜“å‡ºç°æ¼æ´çš„åœ°æ–¹\n",
      "2. æ¼æ´å®¡è®¡ç‚¹\n",
      "2.1. URLConnection\n",
      "2.2. H\n",
      "Document 5 content: page_content='1\n",
      "SSRFæ¼æ´\n",
      "1. SSRFæ¼æ´\n",
      "1.1. åŸç†\n",
      "1.2. æ¼æ´å±å®³\n",
      "1.3. å®¹æ˜“å‡ºç°æ¼æ´çš„åœ°æ–¹\n",
      "2. æ¼æ´å®¡è®¡ç‚¹\n",
      "2.1. URLConnection\n",
      "2.2. H\n"
     ]
    }
   ],
   "source": [
    "def recall_documents(query, index, k=5):\n",
    "    \"\"\"\n",
    "    A similarity retrieval is performed and the k most similar documents to the query are returned\n",
    "    \"\"\"\n",
    "    return index.similarity_search(query, k=k)\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "query = \"2024å¹´ä¸–ç•ŒèŒä¸šé™¢æ ¡åˆ†ä¸ºå“ªäº›èµ›é“?\"\n",
    "retrieved_docs = recall_documents(query, index, k=5)\n",
    "\n",
    "# æ‰“å°æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1} content:\", doc.page_content[:100])  # æ‰“å°æ¯ä¸ªæ–‡æ¡£çš„å‰100ä¸ªå­—ç¬¦\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ec6ba",
   "metadata": {},
   "source": [
    "é‡æ’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c11b1080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API key loaded: sk-or...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆ.env æ‰€åœ¨ç›®å½•ï¼‰\n",
    "root_dir = Path().resolve().parent\n",
    "\n",
    "# åŠ è½½é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶ä¸­çš„ OPENROUTER_API_KEY\n",
    "load_dotenv(dotenv_path=root_dir / \".env\")\n",
    "\n",
    "# æµ‹è¯•è¾“å‡º\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in .env!\")\n",
    "print(\"âœ… API key loaded:\", api_key[:5] + \"...\")    # ğŸ‘ˆ çœ‹çœ‹æ˜¯ä¸æ˜¯ None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "064bf401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked Document 1 content: page_content='2024 å¹´ä¸–ç•ŒèŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›\n",
      "åˆ¶åº¦æ±‡ç¼–\n",
      "ä¸–ç•ŒèŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›æ‰§è¡Œå§”å‘˜ä¼šï¼ˆç­¹ï¼‰\n",
      "2024 å¹´9 æœˆ\n",
      "ç›®\n",
      "å½•\n",
      "ä¸–ç•ŒèŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›ç®¡ç†è§„å®šä¸åŠæ³•\n",
      "ç»„ç»‡æœºæ„ä¸èŒèƒ½åˆ†å·¥....\n",
      "Reranked Document 2 content: page_content='1\n",
      "SSRFæ¼æ´\n",
      "1. SSRFæ¼æ´\n",
      "1.1. åŸç†\n",
      "1.2. æ¼æ´å±å®³\n",
      "1.3. å®¹æ˜“å‡ºç°æ¼æ´çš„åœ°æ–¹\n",
      "2. æ¼æ´å®¡è®¡ç‚¹\n",
      "2.1. URLConnection\n",
      "2.2. H\n",
      "Reranked Document 3 content: page_content='1\n",
      "SSRFæ¼æ´\n",
      "1. SSRFæ¼æ´\n",
      "1.1. åŸç†\n",
      "1.2. æ¼æ´å±å®³\n",
      "1.3. å®¹æ˜“å‡ºç°æ¼æ´çš„åœ°æ–¹\n",
      "2. æ¼æ´å®¡è®¡ç‚¹\n",
      "2.1. URLConnection\n",
      "2.2. H\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# åŠ è½½é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶ä¸­çš„ OPENROUTER_API_KEY\n",
    "root_dir = Path().resolve().parent\n",
    "load_dotenv(dotenv_path=root_dir / \".env\")\n",
    "# ä» .env è¯»å– OPENROUTER_API_KEYï¼Œå¹¶è®¾ç½®æˆ OpenAI å…¼å®¹å˜é‡\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# è·å– OPENROUTER_API_KEY ç¯å¢ƒå˜é‡\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "def rerank_documents(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Reorder retrieved documents using LLM or models\n",
    "    \"\"\"\n",
    "    \n",
    "    # ä½¿ç”¨ChatOpenAIè¿›è¡Œé‡æ’\n",
    "    llm = ChatOpenAI(openai_api_key=api_key, model=\"openai/gpt-4o-mini\")\n",
    "    scores = []\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆç›¸å…³æ€§åˆ†æ•°\n",
    "    for doc in retrieved_docs:\n",
    "        prompt = f\"Query: {query}\\nDocument: {doc.page_content}\\n\"\n",
    "        \n",
    "        # æ„å»ºé€‚åˆChatOpenAIçš„æ¶ˆæ¯æ ¼å¼\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant helping to rank documents based on relevance.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # è·å–ç›¸å…³æ€§åˆ†æ•°ï¼Œç¡®ä¿ä¼ é€’æ­£ç¡®çš„æ¶ˆæ¯åˆ—è¡¨æ ¼å¼\n",
    "        response = llm.invoke(messages)  # ç¡®ä¿ä½¿ç”¨ invoke æ¥è°ƒç”¨\n",
    "        \n",
    "        # æå–æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ï¼ˆç›´æ¥è®¿é—® content å±æ€§ï¼‰\n",
    "        score = response.content  # è·å–ç”Ÿæˆçš„æ–‡æœ¬\n",
    "        \n",
    "        scores.append((score, doc))\n",
    "    \n",
    "    # æŒ‰ç…§åˆ†æ•°å¯¹æ–‡æ¡£è¿›è¡Œæ’åº\n",
    "    sorted_docs = sorted(scores, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # è¿”å›æ’åºåçš„æ–‡æ¡£\n",
    "    return [doc[1] for doc in sorted_docs]\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹ï¼šå‡è®¾ä½ å·²ç»è°ƒç”¨äº† recall_documents è·å–äº† retrieved_docs\n",
    "query = \"2024å¹´ä¸–ç•ŒèŒä¸šé™¢æ ¡åˆ†ä¸ºå“ªäº›èµ›é“?\"\n",
    "retrieved_docs = recall_documents(query, index, k=3)\n",
    "\n",
    "# é‡æ’æ£€ç´¢åˆ°çš„æ–‡æ¡£\n",
    "reranked_docs = rerank_documents(query, retrieved_docs)\n",
    "\n",
    "# æ‰“å°é‡æ’åçš„æ–‡æ¡£å†…å®¹\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"Reranked Document {i+1} content:\", doc.page_content[:100])  # æ‰“å°æ¯ä¸ªæ–‡æ¡£çš„å‰100ä¸ªå­—ç¬¦\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a13553",
   "metadata": {},
   "source": [
    "ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a824d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for query: 2024å¹´ä¸–ç•ŒèŒä¸šé™¢æ ¡åˆ†ä¸ºå“ªäº›èµ›é“?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: 2024å¹´ä¸–ç•ŒèŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›å…±è®¾ç½®42ä¸ªèµ›é“ã€‚è¿™äº›èµ›é“æ¶‰åŠä¸åŒçš„ä¸“ä¸šé¢†åŸŸï¼Œå…·ä½“çš„èµ›é“ä¿¡æ¯å¯èƒ½ä¼šåœ¨ç›¸å…³çš„èµ›äº‹é€šçŸ¥æˆ–æŒ‡å—ä¸­è¯¦ç»†åˆ—å‡ºã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# åŠ è½½é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶ä¸­çš„ OPENROUTER_API_KEY\n",
    "root_dir = Path().resolve().parent\n",
    "load_dotenv(dotenv_path=root_dir / \".env\")\n",
    "\n",
    "# ä» .env è¯»å– OPENROUTER_API_KEYï¼Œå¹¶è®¾ç½®æˆ OpenAI å…¼å®¹å˜é‡\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# è·å– OPENROUTER_API_KEY ç¯å¢ƒå˜é‡\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "def generate_answer(query, top_docs):\n",
    "    \"\"\"\n",
    "    åŸºäºæŸ¥è¯¢å’Œé‡æ’åçš„æ–‡æ¡£ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    # å°†æ–‡æ¡£å†…å®¹æ‹¼æ¥ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆç¡®ä¿ä¼ é€’ç»™æ¨¡å‹çš„æ˜¯ä¸€ä¸ªé•¿æ–‡æœ¬ï¼‰\n",
    "    documents_content = \"\\n\".join([doc.page_content for doc in top_docs])\n",
    "    \n",
    "    # æ ¼å¼åŒ–promptï¼Œç¡®ä¿ä¼ é€’ç»™æ¨¡å‹çš„æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²\n",
    "    prompt = f\"Answer the following question based on the retrieved documents:\\n{documents_content}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # ä½¿ç”¨ChatOpenAIè¿›è¡Œç”Ÿæˆ\n",
    "    llm = ChatOpenAI(openai_api_key=api_key, model=\"openai/gpt-4o-mini\")\n",
    "    \n",
    "    try:\n",
    "        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ\n",
    "        response = llm.invoke([{\"role\": \"system\", \"content\": \"You are an assistant helping to answer questions based on documents.\"},\n",
    "                               {\"role\": \"user\", \"content\": prompt}])\n",
    "        \n",
    "        # è·å–ç”Ÿæˆçš„ç­”æ¡ˆï¼ˆé€šå¸¸æ˜¯ä» response.content ä¸­æå–ï¼‰\n",
    "        answer = response.content.strip()  # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå»é™¤å¤šä½™çš„ç©ºæ ¼\n",
    "        \n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹ï¼šå‡è®¾ä½ å·²ç»è°ƒç”¨äº† recall_documents è·å–äº† retrieved_docs å’Œ reranked_docs\n",
    "query = \"2024å¹´ä¸–ç•ŒèŒä¸šé™¢æ ¡åˆ†ä¸ºå“ªäº›èµ›é“?\"\n",
    "print(\"Generating answer for query:\", query)\n",
    "\n",
    "# è°ƒç”¨ç”Ÿæˆç­”æ¡ˆçš„å‡½æ•°\n",
    "answer = generate_answer(query, reranked_docs)\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„ç­”æ¡ˆ\n",
    "if answer:\n",
    "    print(\"Generated answer:\", answer)\n",
    "else:\n",
    "    print(\"Failed to generate an answer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e07cfe",
   "metadata": {},
   "source": [
    "é›†æˆ FastMCP åˆ›å»º MCP æœåŠ¡å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22a15c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. FastMCP will be managed automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/queen/Documents/VSCode/llm_retrieval/.venv/lib/python3.13/site-packages/fastmcp/server/server.py:213: DeprecationWarning: Providing `log_level` when creating a server is deprecated. Provide it when calling `run` or as a global setting instead.\n",
      "  self._handle_deprecated_settings(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "# åŠ è½½é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶ä¸­çš„ OPENROUTER_API_KEY\n",
    "root_dir = Path().resolve().parent\n",
    "load_dotenv(dotenv_path=root_dir / \".env\")\n",
    "\n",
    "# è·å– OPENROUTER_API_KEY ç¯å¢ƒå˜é‡\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# å®šä¹‰çŸ¥è¯†åº“è·¯å¾„\n",
    "knowledge_base_folder = \"/Users/queen/Documents/VSCode/llm_retrieval/Local_knowledge_base\"\n",
    "\n",
    "# åˆ›å»º FastMCP å®ä¾‹\n",
    "mcp = FastMCP(\"localRetrieval\", log_level=\"ERROR\")\n",
    "\n",
    "@mcp.tool(name=\"doc_init\", description=\"A server for handling document retrieval and answer generation\")\n",
    "async def doc_init(self, knowledge_base_folder):\n",
    "    \"\"\"\n",
    "    å¼‚æ­¥æ–‡æ¡£åˆå§‹åŒ–ï¼Œæ„å»ºæ–‡æ¡£ç´¢å¼•\n",
    "    \"\"\"\n",
    "    self.index = build_index(knowledge_base_folder)  # æ„å»ºæ–‡æ¡£ç´¢å¼•\n",
    "\n",
    "@mcp.tool(name=\"handle_query\", description=\"Handle a query by recalling, reranking documents and generating an answer\")\n",
    "async def handle_query(self, query):\n",
    "    \"\"\"\n",
    "    å¤„ç†æŸ¥è¯¢ï¼Œå®Œæˆæ–‡æ¡£å¬å›ã€é‡æ’ä¸ç­”æ¡ˆç”Ÿæˆçš„è¿‡ç¨‹\n",
    "    \"\"\"\n",
    "    # Step 1: å¬å›æ–‡æ¡£\n",
    "    retrieved_docs = await recall_documents(query, self.index)  # å¼‚æ­¥å¬å›æ–‡æ¡£\n",
    "    \n",
    "    # Step 2: é‡æ’æ–‡æ¡£\n",
    "    reranked_docs = await rerank_documents(query, retrieved_docs)  # å¼‚æ­¥é‡æ’æ–‡æ¡£\n",
    "    \n",
    "    # Step 3: ç”Ÿæˆç­”æ¡ˆ\n",
    "    answer = await generate_answer(query, reranked_docs)  # å¼‚æ­¥ç”Ÿæˆç­”æ¡ˆ\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# ç¡®ä¿ mcp.run() è¿è¡Œæ—¶ä¸ä¸å·²æœ‰äº‹ä»¶å¾ªç¯å†²çª\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦å·²æœ‰äº‹ä»¶å¾ªç¯åœ¨è¿è¡Œ\n",
    "    if not asyncio.get_event_loop().is_running():\n",
    "        # å¦‚æœæ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥è°ƒç”¨ run()\n",
    "        mcp.run(transport='stdio')\n",
    "    else:\n",
    "        # å¦‚æœå·²ç»æœ‰äº‹ä»¶å¾ªç¯è¿è¡Œï¼ŒFastMCP è‡ªåŠ¨ç®¡ç†äº‹ä»¶å¾ªç¯\n",
    "        print(\"Event loop is already running. FastMCP will be managed automatically.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
