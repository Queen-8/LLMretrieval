# src/langchainagenticai/agent/agent.py
import os
import json
import re
import logging
from string import Template
from typing import Any, Dict, List

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_mcp_adapters.client import MultiServerMCPClient

from .prompt_template import REACT_SYSTEM_PROMPT

logger = logging.getLogger(__name__)

def _render_system_prompt(mcp_tools: List[Any]) -> str:
    tool_lines = []
    for t in mcp_tools:
        sig = ""
        if getattr(t, "input_schema", None):
            try:
                sig = json.dumps(t.input_schema, ensure_ascii=False)
            except Exception:
                sig = str(t.input_schema)
        desc = getattr(t, "description", "") or ""
        tool_lines.append(f"- {t.name}: {desc}\n  schema: {sig}")

    file_list = ", ".join(os.listdir(os.getcwd()))
    system = Template(REACT_SYSTEM_PROMPT).substitute(
        tool_list="\n".join(tool_lines),
        operating_system=os.name,
        file_list=file_list,
    )
    return system


def _coerce_arguments_for_tool(tool: Any, arguments: Any) -> Dict[str, Any]:
    """
    ä¾æ® MCP å·¥å…·çš„ input_schema è‡ªåŠ¨â€œçº æ­£â€å‚æ•°å½¢çŠ¶ã€‚
    å…¸å‹åœºæ™¯ï¼šå·¥å…·è¦æ±‚é¡¶å±‚ { "input_data": {...} }ï¼Œè€Œæ¨¡å‹ç»™äº† {...}
    """
    # æœªæä¾› schema å°±åŸæ ·è¿”å›
    schema = getattr(tool, "input_schema", None)
    if not isinstance(schema, dict):
        return arguments if isinstance(arguments, dict) else {"input_data": {"query": str(arguments)}}

    required = set(schema.get("required", []))
    props = schema.get("properties", {})

    # å·¥å…·è¦æ±‚å¿…é¡»æœ‰ input_data
    needs_input_data = ("input_data" in required) or ("input_data" in props)

    # å¦‚æœéœ€è¦ input_dataï¼Œä½†å½“å‰æ²¡æœ‰ï¼Œå°±å¸®å¿™åŒ…è£¹
    if needs_input_data and (not isinstance(arguments, dict) or "input_data" not in arguments):
        if isinstance(arguments, dict):
            # å¸¸è§æƒ…å†µï¼šæ¨¡å‹ç»™äº† {"query": "..."} â†’ åŒ…æˆ {"input_data": {"query": "..."}}
            return {"input_data": arguments}
        else:
            # æ¨¡å‹ç»™äº†å­—ç¬¦ä¸²/åˆ«çš„ â†’ å°è¯•ä½œä¸º query
            return {"input_data": {"query": str(arguments)}}

    # å¦åˆ™åŸæ ·è¿”å›
    return arguments if isinstance(arguments, dict) else {"input_data": {"query": str(arguments)}}


class MCPAgent:
    """
    ReAct + MCPï¼š
    - <thought>/<action>ï¼ˆJSONï¼‰ â†’ è°ƒç”¨ MCP tool â†’ <observation> â†’ â€¦ â†’ <final_answer>
    - æ‰“å°å…³é”®æ—¥å¿—ï¼›è®¾ç½®æœ€å¤§æ­¥æ•°é¿å…æ­»å¾ªç¯
    """

    def __init__(self, model: BaseChatModel, mcp_client: MultiServerMCPClient, max_steps: int = 8, verbose: bool = True):
        self.model = model
        self.mcp_client = mcp_client
        self.max_steps = max_steps
        self.verbose = verbose

    async def ainvoke(self, user_query: str) -> str:
        tools = await self.mcp_client.get_tools()
        if not tools:
            return "âŒ æœªå‘ç°ä»»ä½• MCP å·¥å…·ï¼Œè¯·æ£€æŸ¥ mcp_server æ˜¯å¦å¯åŠ¨ã€è¿æ¥é”®åæ˜¯å¦åŒ¹é…ã€‚"

        system_prompt = _render_system_prompt(tools)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"<question>{user_query}</question>"},
        ]

        used_tool_once = False
        step = 0

        while True:
            step += 1
            if step > self.max_steps:
                logger.warning("ğŸ” å·²è¾¾åˆ°æœ€å¤§æ­¥æ•°ä¸Šé™ï¼Œå¼ºåˆ¶ç»“æŸã€‚")
                return "âš ï¸ è¶…è¿‡æœ€å¤§æ¨ç†æ­¥æ•°ï¼Œæ¨¡å‹æœªèƒ½åœ¨é™åˆ¶å†…å®Œæˆã€‚è¯·é‡è¯•æˆ–æ¢ä¸ªé—®é¢˜ã€‚"

            # 1) è°ƒæ¨¡å‹
            ai_msg = await self.model.ainvoke(messages)
            content = ai_msg.content if isinstance(ai_msg.content, str) else str(ai_msg.content)
            messages.append({"role": "assistant", "content": content})

            # æ‰“å° thought
            m_th = re.search(r"<thought>(.*?)</thought>", content, re.DOTALL)
            if m_th:
                logger.info(f"[Step {step}] ğŸ’­ Thought: {m_th.group(1).strip()}")

            # 2) final_answerï¼Ÿ
            if "<final_answer>" in content:
                if not used_tool_once:
                    warn = "é”™è¯¯ï¼šä½ å°šæœªè°ƒç”¨ä»»ä½•å·¥å…·ã€‚è¯·å…ˆè¾“å‡º <action> å¹¶è°ƒç”¨ä¸€ä¸ª MCP å·¥å…·ã€‚"
                    logger.info(f"[Step {step}] â›” {warn}")
                    messages.append({"role": "user", "content": f"<observation>{warn}</observation>"})
                    continue
                m = re.search(r"<final_answer>(.*?)</final_answer>", content, re.DOTALL)
                final_answer = m.group(1).strip() if m else ""
                logger.info(f"[Step {step}] âœ… Final Answer: {final_answer[:200]}{'...' if len(final_answer)>200 else ''}")
                return final_answer or "ï¼ˆæœªèƒ½è§£æ <final_answer> å†…å®¹ï¼‰"

            # 3) è§£æ <action>ï¼ˆJSONï¼‰
            m = re.search(r"<action>(.*?)</action>", content, re.DOTALL)
            if not m:
                err = "é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° <action> æˆ– <final_answer>ã€‚è¯·æŒ‰åè®®è¾“å‡ºã€‚"
                logger.info(f"[Step {step}] â›” {err}")
                messages.append({"role": "user", "content": f"<observation>{err}</observation>"})
                continue

            action_block = m.group(1).strip()
            logger.info(f"[Step {step}] ğŸ”§ Raw <action>: {action_block}")

            try:
                action_obj = json.loads(action_block)
                tool_name = action_obj["tool_name"]
                arguments = action_obj.get("arguments", {})
            except Exception as e:
                err = f"é”™è¯¯ï¼š<action> å¿…é¡»æ˜¯ JSONã€‚è§£æå¤±è´¥ï¼š{e}"
                logger.info(f"[Step {step}] â›” {err}")
                messages.append({"role": "user", "content": f"<observation>{err}</observation>"})
                continue

            # 4) åŒ¹é… MCP tool
            tool = next((t for t in tools if t.name == tool_name), None)
            if tool is None:
                err = f"é”™è¯¯ï¼šæœªæ‰¾åˆ°åä¸º {tool_name} çš„ MCP å·¥å…·ã€‚"
                logger.info(f"[Step {step}] â›” {err}")
                messages.append({"role": "user", "content": f"<observation>{err}</observation>"})
                continue

            # â­ 5) **Schema è‡ªé€‚é…**ï¼šæ ¹æ® tool.input_schema çº æ­£å‚æ•°å½¢çŠ¶ï¼ˆæ¯”å¦‚åŒ…è£¹ input_dataï¼‰
            coerced_args = _coerce_arguments_for_tool(tool, arguments)
            logger.info(f"[Step {step}] ğŸ› ï¸ Tool: {tool_name} | Args: {json.dumps(coerced_args, ensure_ascii=False)}")

            # 6) è°ƒç”¨ MCP tool
            try:
                result = await tool.ainvoke(coerced_args)
                used_tool_once = True
            except Exception as e:
                result = f"å·¥å…·æ‰§è¡Œå¼‚å¸¸ï¼š{e}"

            # 7) Observation
            show = result if isinstance(result, str) else json.dumps(result, ensure_ascii=False)
            logger.info(f"[Step {step}] ğŸ” Observation: {show[:600]}{' ...[truncated]' if len(show)>600 else ''}")

            messages.append({"role": "user", "content": f"<observation>{show}</observation>"})


async def create_mcp_agent(model: BaseChatModel, connections: Dict[str, Any]) -> MCPAgent:
    mcp_client = MultiServerMCPClient(connections=connections)
    await mcp_client.get_tools()  # é¢„çƒ­
    return MCPAgent(model=model, mcp_client=mcp_client, max_steps=8, verbose=True)