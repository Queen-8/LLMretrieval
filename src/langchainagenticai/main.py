import os
import sys
import asyncio
import streamlit as st
from pydantic import SecretStr
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchainagenticai.utils.loaddocs import init_retrieval_pipeline
from langchainagenticai.utils.loadurls import init_url_retrieval_pipeline
from langchainagenticai.ui.streamlit.load_ui import LoadStreamlitUI
from langchainagenticai.ui.streamlit.display_result import DisplayResultStreamlit
from langchain_openai import ChatOpenAI
from langchainagenticai.mcp_server.local_server import local_answer_question
from langchainagenticai.mcp_server.cloud_server import cloud_answer_question
from pydantic import SecretStr
import json
import requests       
from langchain_openai import OpenAI

from langchain_community.chat_models import ChatOllama
from langchain.chat_models import ChatOllama
import logging

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
project_root = os.path.abspath(os.path.join(os.getcwd(), "../../.."))  # è·³å‡º src/langchainagenticai/retrieval
sys.path.append(project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")



# MCP å®¢æˆ·ç«¯åˆå§‹åŒ–ï¼ˆå…¨å±€åªåˆ›å»ºä¸€æ¬¡ï¼‰
mcp_clients = {
    "CloudRetrievalQA": MultiServerMCPClient(
        connections={
            "CloudRetrievalQA": {
                "command": "python",
                "args": ["src/langchainagenticai/mcp_server/cloud_server.py"],
                "transport": "stdio",
            }
        }
    ),
    "LocalRetrievalQA": MultiServerMCPClient(
        connections={
            "LocalRetrievalQA": {
                "command": "python",
                "args": ["src/langchainagenticai/mcp_server/local_server.py"],
                "transport": "stdio",
            }
        }
    ),
}

async def ask_cloud_tool(client, query):
    tools = await client.get_tools()
    cloud_tool = next((t for t in tools if t.name == "cloud_answer_question"), None)
    if not cloud_tool:
        return "âŒ æœªæ‰¾åˆ° cloud_answer_question å·¥å…·"
    result = await cloud_tool.ainvoke({"query": query})
    return result

async def ask_local_tool(client, query):
    tools = await client.get_tools()
    local_tool = next((t for t in tools if t.name == "local_answer_question"), None)
    if not local_tool:
        return "âŒ æœªæ‰¾åˆ° local_answer_question å·¥å…·"
    result = await local_tool.ainvoke({"query": query})
    return result

def load_langgraph_agenticai_app():
    st.title("ğŸ¤– LangChain Agentic AI")

    # åˆå§‹åŒ–çŸ¥è¯†åº“ç´¢å¼•ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    if "vector_index" not in st.session_state:
        with st.spinner("ğŸ“‚ æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°çŸ¥è¯†åº“ç´¢å¼•..."):
            try:
                st.session_state.vector_index = init_retrieval_pipeline()
                st.success("ğŸ“‚âœ… æœ¬åœ°çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
            except Exception as e:
                st.error(f"âŒ æœ¬åœ°çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
                return

    if "vector_index_urls" not in st.session_state:
        with st.spinner("ğŸŒ æ­£åœ¨åˆå§‹åŒ– URL çŸ¥è¯†åº“ç´¢å¼•..."):
            try:
                st.session_state.vector_index_urls = init_url_retrieval_pipeline()
                st.success("ğŸŒâœ… URL çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
            except Exception as e:
                st.error(f"âŒ URL çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")

    # åˆå§‹åŒ–æ¶ˆæ¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("æ¨¡å‹é…ç½®")
        
        # ä½¿ç”¨ LoadStreamlitUI æ¥æ˜¾ç¤ºå®Œæ•´çš„é…ç½®ç•Œé¢
        ui = LoadStreamlitUI(set_page_config=False)
        st.session_state.user_input = ui.load_streamlit_ui()

        st.header("å¯¹è¯ç®¡ç†")
        user_messages = len([msg for msg in st.session_state.messages if msg["role"] == "user"])
        assistant_messages = len([msg for msg in st.session_state.messages if msg["role"] == "assistant"])
        st.metric("ç”¨æˆ·æ¶ˆæ¯", user_messages)
        st.metric("åŠ©æ‰‹å›å¤", assistant_messages)

        if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
            st.session_state.messages = []
            st.rerun()
            
        if st.session_state.messages:
            if st.button("ğŸ“¥ å¯¼å‡ºå¯¹è¯"):
                conversation_text = ""
                for msg in st.session_state.messages:
                    role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
                    conversation_text += f"{role}: {msg['content']}\n\n"

                st.download_button(
                    label="ä¸‹è½½å¯¹è¯è®°å½•",
                    data=conversation_text,
                    file_name=f"conversation_{st.session_state.get('session_id', 'unknown')}.txt",
                    mime="text/plain"
                )

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()

            try:
                user_input = st.session_state.user_input
                selected_llm = user_input.get("selected_llm", "ollama")

                # ç”±äº Streamlit æœ¬èº«æ˜¯åŒæ­¥è¿è¡Œï¼Œè°ƒç”¨å¼‚æ­¥è¦ç”¨ asyncio.run æˆ– asyncio.create_task
                # ä½† asyncio.run åœ¨ Streamlit é‡Œå¯èƒ½æœ‰äº‹ä»¶å¾ªç¯å†²çªï¼Œæ¨èä½¿ç”¨ asyncio.run if no running loop
                # è¿™é‡Œåšç®€å•å¤„ç†ï¼š

                if selected_llm == "OpenAI":
                    client = mcp_clients["CloudRetrievalQA"]
                    answer = asyncio.run(ask_cloud_tool(client, prompt))
                else:
                    client = mcp_clients["LocalRetrievalQA"]
                    answer = asyncio.run(ask_local_tool(client, prompt))

                message_placeholder.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                error_msg = f"âŒ å‘ç”Ÿé”™è¯¯: {e}"
                logging.error(error_msg)
                message_placeholder.markdown(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})


# # åˆå§‹åŒ– MCP å®¢æˆ·ç«¯ï¼ˆå…¨å±€ï¼Œåªåˆ›å»ºä¸€æ¬¡ï¼‰
# mcp_clients = {
#     "CloudRetrievalQA": MultiServerMCPClient(
#         connections={
#             "CloudRetrievalQA": {
#                 "command": "python",
#                 "args": ["src/langchainagenticai/mcp_server/cloud_server.py"],
#                 "transport": "stdio",
#             }
#         }
#     ),
#     "LocalRetrievalQA": MultiServerMCPClient(
#         connections={
#             "LocalRetrievalQA": {
#                 "command": "python",
#                 "args": ["src/langchainagenticai/mcp_server/local_server.py"],
#                 "transport": "stdio",
#             }
#         }
#     ),
# }





# def load_langgraph_agenticai_app():
#     st.title("ğŸ¤– LangChain Agentic AI")

#     # åˆå§‹åŒ–å‘é‡ç´¢å¼•ï¼Œä»…åœ¨é¦–æ¬¡åŠ è½½æ—¶æ‰§è¡Œä¸€æ¬¡
#     if "vector_index" not in st.session_state:
#         with st.spinner("ğŸ“‚ æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°çŸ¥è¯†åº“ç´¢å¼•..."):
#             try:
#                 st.session_state.vector_index = init_retrieval_pipeline()
#                 st.success("ğŸ“‚âœ… æœ¬åœ°çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
#             except Exception as e:
#                 st.error(f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
#                 return  # è‹¥åˆå§‹åŒ–å¤±è´¥ï¼Œç»ˆæ­¢å¯åŠ¨æµç¨‹
#     if "vector_index_urls" not in st.session_state:
#         with st.spinner("ğŸŒ æ­£åœ¨åˆå§‹åŒ– URL çŸ¥è¯†åº“ç´¢å¼•..."):
#             try:
#                 st.session_state.vector_index_urls = init_url_retrieval_pipeline()
#                 st.success("ğŸŒâœ… URL çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
#             except Exception as e:
#                 st.error(f"âŒ URL çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
#     # åˆå§‹åŒ–æ¶ˆæ¯å†å²
#     if "messages" not in st.session_state:
#         st.session_state.messages = []
    
#     # ä¾§è¾¹æ åŠŸèƒ½
#     with st.sidebar:
#         st.header("æ¨¡å‹é…ç½®")
        
#         # åŠ è½½UIé…ç½®
#         ui = LoadStreamlitUI()
#         user_input = ui.load_streamlit_ui()
        
#         st.header("å¯¹è¯ç®¡ç†")
        
#         # æ˜¾ç¤ºæ¶ˆæ¯ç»Ÿè®¡
#         user_messages = len([msg for msg in st.session_state.messages if msg["role"] == "user"])
#         assistant_messages = len([msg for msg in st.session_state.messages if msg["role"] == "assistant"])
#         st.metric("ç”¨æˆ·æ¶ˆæ¯", user_messages)
#         st.metric("åŠ©æ‰‹å›å¤", assistant_messages)
        
#         # æ¸…é™¤å†å²æŒ‰é’®
#         if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
#             st.session_state.messages = []
#             st.rerun()
        
#         # å¯¼å‡ºå¯¹è¯åŠŸèƒ½
#         if st.session_state.messages:
#             if st.button("ğŸ“¥ å¯¼å‡ºå¯¹è¯"):
#                 # ç”Ÿæˆå¯¹è¯æ–‡æœ¬
#                 conversation_text = ""
#                 for msg in st.session_state.messages:
#                     role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
#                     conversation_text += f"{role}: {msg['content']}\n\n"
                
#                 # æä¾›ä¸‹è½½
#                 st.download_button(
#                     label="ä¸‹è½½å¯¹è¯è®°å½•",
#                     data=conversation_text,
#                     file_name=f"conversation_{st.session_state.get('session_id', 'unknown')}.txt",
#                     mime="text/plain"
#                 )
    
#     # æ˜¾ç¤ºå†å²æ¶ˆæ¯
#     for message in st.session_state.messages:
#         with st.chat_message(message["role"]):
#             st.markdown(message["content"])
    
#     # ç”¨æˆ·è¾“å…¥
#     if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):

#         # âœ… ä¼˜å…ˆæ„é€  RAG ä¸Šä¸‹æ–‡
#         if "vector_index" in st.session_state:
#         # ä½¿ç”¨å‘é‡ç´¢å¼•åš semantic search
#             rag_results = st.session_state.vector_index.similarity_search(prompt, k=3)
#             context = "\n".join([doc.page_content for doc in rag_results])
#             prompt_with_context = f"æ ¹æ®ä»¥ä¸‹çŸ¥è¯†å†…å®¹å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜ï¼š{prompt}"
#         else:
#             prompt_with_context = prompt

#         # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
#         st.session_state.messages.append({"role": "user", "content": prompt})
        
#         # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
#         with st.chat_message("user"):
#             st.markdown(prompt)
        
#         # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯
#         with st.chat_message("assistant"):
#             message_placeholder = st.empty()
            
#             try:
#                 # è·å–é€‰æ‹©çš„æ¨¡å‹ä¿¡æ¯
#                 selected_llm = user_input.get("selected_llm")
#                 selected_model = user_input.get("selected_model")
                
#                 if selected_llm == "OpenAI":
#                     api_key = (
#                         user_input.get("OPENAI_API_KEY")
#                         or os.getenv("OPENAI_API_KEY")
#                         or os.getenv("OPENROUTER_API_KEY")
#                     )
#                     if not api_key:
#                         st.error("âš ï¸ è¯·æä¾› OpenAI API Key")
#                         return
                    
#                     # åˆå§‹åŒ– MCP å®¢æˆ·ç«¯
#                     client = MultiServerMCPClient(
#                         connections={
#                             "CloudRetrievalQA": {
#                                 "command": "python",
#                                 "args": ["src/langchainagenticai/mcp_server/cloud_server.py"],
#                                 "transport": "stdio",
#                             }
#                         }
#                     )
#                     async def ask_cloud_tool(history, context, api_key):
#                         tools = await client.get_tools()
#                         cloud_tool = next((t for t in tools if t.name == "cloud_answer_question"), None)
#                         if not cloud_tool:
#                             return "âŒ æœªæ‰¾åˆ° cloud_answer_question å·¥å…·"
#                         # ä¼ é€’ api_key ä½œä¸ºå‚æ•°
#                         result = await cloud_tool.ainvoke({"history": history, "context": context, "api_key": api_key})
#                         return result

#                     # è°ƒç”¨ç¤ºä¾‹ï¼ˆåŒæ­¥ï¼‰
#                     answer = asyncio.run(ask_cloud_tool(st.session_state.messages, context, api_key))
#                     message_placeholder.markdown(answer)
#                     st.session_state.messages.append({"role": "assistant", "content": answer})
                    
#                 elif selected_llm == "ollama":

#                     # åˆå§‹åŒ– MCP å®¢æˆ·ç«¯
#                     client = MultiServerMCPClient(
#                         connections={
#                             "LocalRetrievalQA": {
#                                 "command": "python",
#                                 "args": ["src/langchainagenticai/mcp_server/local_server.py"],
#                                 "transport": "stdio",
#                             }
#                         }
#                     )

#                     async def ask_local_tool(query: str) -> str:
#                         # è·å–å·¥å…·
#                         tools = await client.get_tools()
#                         # æ‰¾åˆ° local_answer_question å·¥å…·
#                         local_tool = None
#                         for tool in tools:
#                             if tool.name == "local_answer_question":
#                                 local_tool = tool
#                                 break
                        
#                         if local_tool:
#                             result = await local_tool.ainvoke({"query": query})
#                             return result
#                         else:
#                             return "âŒ æœªæ‰¾åˆ° local_answer_question å·¥å…·"

#                     # åœ¨ Streamlit æˆ–å…¶ä»–å‰ç«¯ä¸­è°ƒç”¨
#                     answer = asyncio.run(ask_local_tool(prompt))

#                     message_placeholder.markdown(answer)
                    
#                     # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
#                     st.session_state.messages.append({"role": "assistant", "content": answer})
                    
#                 else:
#                     st.error(f"âš ï¸ ä¸æ”¯æŒçš„LLMç±»å‹: {selected_llm}")
                
                    
#             except Exception as e:
#                 error_message = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
#                 message_placeholder.markdown(error_message)
#                 st.session_state.messages.append({"role": "assistant", "content": error_message})
