import os
import sys
import asyncio
import logging
import streamlit as st
from dotenv import load_dotenv

# ====== è¿è¡ŒæœŸç¯å¢ƒï¼ˆå°½é‡æœ€å‰ï¼‰======
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("USER_AGENT", "LangChain-AgenticAI/1.0 (contact: you@example.com)")

load_dotenv(override=True)

# ç»™åªè®¤ OPENAI_* çš„åº“åšåˆ«åï¼ˆå…œåº•ï¼‰
if os.getenv("OPENROUTER_API_KEY") and not os.getenv("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENROUTER_API_KEY")
os.environ.setdefault("OPENAI_BASE_URL", os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1"))

# ====== Importsï¼ˆæ”¾åœ¨ç¯å¢ƒå‡†å¤‡ä¹‹åï¼‰======
from langchain_community.chat_models import ChatOllama
from langchain_openai import ChatOpenAI
from langchainagenticai.agent.agent import create_mcp_agent
from langchainagenticai.utils.loaddocs import init_retrieval_pipeline
from langchainagenticai.utils.loadurls import init_url_retrieval_pipeline
from langchainagenticai.ui.streamlit.load_ui import LoadStreamlitUI

# é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.abspath(os.path.join(os.getcwd(), "../../.."))
sys.path.append(project_root)

# æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# MCP è¿æ¥
MCP_CONNECTIONS = {
    "LocalRetrievalQA": {
        "command": "python",
        "args": ["src/langchainagenticai/mcp_server/local_server.py"],
        "transport": "stdio",
    },
    "CloudRetrievalQA": {
        "command": "python",
        "args": ["src/langchainagenticai/mcp_server/cloud_server.py"],
        "transport": "stdio",
    },
}

# --------- æ„å»º/ç¼“å­˜ LLM & Agent ----------
def _build_llm(selected_llm: str):
    selected_llm = (selected_llm or "ollama").strip()
    if selected_llm.lower() == "openai":
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise RuntimeError("ç¼ºå°‘ OPENROUTER_API_KEYï¼Œè¯·åœ¨ .env ä¸­è®¾ç½®")

        base_url = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
        model_name = os.getenv("OPENROUTER_MODEL", "openai/gpt-4o-mini")

        openrouter_headers = {
            "HTTP-Referer": os.getenv("OR_HTTP_REFERER", "http://localhost"),
            "X-Title": os.getenv("OR_X_TITLE", "LangChain Agentic AI"),
            "User-Agent": os.getenv("USER_AGENT", "LangChain-AgenticAI/1.0"),
        }

        llm = ChatOpenAI(
            model=model_name,
            api_key=api_key,
            base_url=base_url,
            default_headers=openrouter_headers,
            timeout=120,       # é˜²å¡ä½
            max_retries=2,
        )
        logger.info(f"âœ… OpenRouter LLM ready: {model_name}")
        return llm

    llm = ChatOllama(model="llama3.1:8b", base_url="http://localhost:11434")
    logger.info("âœ… Ollama LLM ready: llama3.1:8b")
    return llm


def _get_cached_agent(selected_llm: str):
    key_llm = f"llm::{selected_llm}"
    key_agent = f"mcp_agent::{selected_llm}"

    if key_agent in st.session_state:
        return st.session_state[key_agent]

    llm = st.session_state.get(key_llm)
    if llm is None:
        llm = _build_llm(selected_llm)
        st.session_state[key_llm] = llm

    agent = asyncio.run(create_mcp_agent(llm, MCP_CONNECTIONS))
    st.session_state[key_agent] = agent
    return agent

# --------- UI ä¸»æµç¨‹ ----------
def load_langgraph_agenticai_app():
    st.title("ğŸ¤– LangChain Agentic AI")

    # åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ˆåªåšä¸€æ¬¡ï¼‰
    if "vector_index" not in st.session_state:
        with st.spinner("ğŸ“‚ æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°çŸ¥è¯†åº“ç´¢å¼•..."):
            try:
                st.session_state.vector_index = init_retrieval_pipeline()
                st.success("ğŸ“‚âœ… æœ¬åœ°çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
            except Exception as e:
                st.error(f"âŒ æœ¬åœ°çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
                return

    if "vector_index_urls" not in st.session_state:
        with st.spinner("ğŸŒ æ­£åœ¨åˆå§‹åŒ– URL çŸ¥è¯†åº“ç´¢å¼•..."):
            try:
                st.session_state.vector_index_urls = init_url_retrieval_pipeline()
                st.success("ğŸŒâœ… URL çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
            except Exception as e:
                st.error(f"âŒ URL çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("æ¨¡å‹é…ç½®")
        ui = LoadStreamlitUI(set_page_config=False)
        st.session_state.user_input = ui.load_streamlit_ui()

        st.header("å¯¹è¯ç®¡ç†")
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
            st.session_state.messages = []
            st.rerun()

    # æ˜¾ç¤ºå†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            placeholder = st.empty()
            try:
                user_input = st.session_state.get("user_input", {}) or {}
                selected_llm = user_input.get("selected_llm", "ollama")

                agent = _get_cached_agent(selected_llm)
                answer = asyncio.run(agent.ainvoke(prompt))

                placeholder.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                err = f"âŒ å‘ç”Ÿé”™è¯¯: {e}"
                logging.exception("assistant error")
                placeholder.markdown(err)
                st.session_state.messages.append({"role": "assistant", "content": err})