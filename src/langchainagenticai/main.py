import os
import sys
import asyncio
import logging
import streamlit as st
from dotenv import load_dotenv

# ====== è¿è¡ŒæœŸç¯å¢ƒï¼ˆå°½é‡æœ€å‰ï¼‰======
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("USER_AGENT", "LangChain-AgenticAI/1.0 (contact: you@example.com)")

load_dotenv(override=True)

# ç»™åªè®¤ OPENAI_* çš„åº“åšåˆ«åï¼ˆå…œåº•ï¼‰
if os.getenv("OPENROUTER_API_KEY") and not os.getenv("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENROUTER_API_KEY")
os.environ.setdefault("OPENAI_BASE_URL", os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1"))

# ====== Importsï¼ˆæ”¾åœ¨ç¯å¢ƒå‡†å¤‡ä¹‹åï¼‰======
from langchain_community.chat_models import ChatOllama
from langchain_openai import ChatOpenAI
from langchainagenticai.agent.agent import create_mcp_agent
from langchainagenticai.utils.loaddocs import init_retrieval_pipeline
from langchainagenticai.utils.loadurls import init_url_retrieval_pipeline
from langchainagenticai.ui.streamlit.load_ui import LoadStreamlitUI

# é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.abspath(os.path.join(os.getcwd(), "../../.."))
sys.path.append(project_root)

# æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# MCP è¿æ¥
MCP_CONNECTIONS = {
    "LocalRetrievalQA": {
        "command": "python",
        "args": ["src/langchainagenticai/mcp_server/local_server.py"],
        "transport": "stdio",
    },
    "CloudRetrievalQA": {
        "command": "python",
        "args": ["src/langchainagenticai/mcp_server/cloud_server.py"],
        "transport": "stdio",
    },
}

# --------- æ„å»º/ç¼“å­˜ LLM & Agent ----------
def _build_llm(selected_llm: str):
    selected_llm = (selected_llm or "ollama").strip()
    if selected_llm.lower() == "openai":
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise RuntimeError("ç¼ºå°‘ OPENROUTER_API_KEYï¼Œè¯·åœ¨ .env ä¸­è®¾ç½®")

        base_url = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
        model_name = os.getenv("OPENROUTER_MODEL", "openai/gpt-4o-mini")

        openrouter_headers = {
            "HTTP-Referer": os.getenv("OR_HTTP_REFERER", "http://localhost"),
            "X-Title": os.getenv("OR_X_TITLE", "LangChain Agentic AI"),
            "User-Agent": os.getenv("USER_AGENT", "LangChain-AgenticAI/1.0"),
        }

        llm = ChatOpenAI(
            model=model_name,
            api_key=api_key,
            base_url=base_url,
            default_headers=openrouter_headers,
            timeout=120,       # é˜²å¡ä½
            max_retries=2,
        )
        logger.info(f"âœ… OpenRouter LLM ready: {model_name}")
        return llm

    llm = ChatOllama(model="llama3.1:8b", base_url="http://localhost:11434")
    logger.info("âœ… Ollama LLM ready: llama3.1:8b")
    return llm


def _get_cached_agent(selected_llm: str):
    key_llm = f"llm::{selected_llm}"
    key_agent = f"mcp_agent::{selected_llm}"

    if key_agent in st.session_state:
        return st.session_state[key_agent]

    llm = st.session_state.get(key_llm)
    if llm is None:
        llm = _build_llm(selected_llm)
        st.session_state[key_llm] = llm

    agent = asyncio.run(create_mcp_agent(llm, MCP_CONNECTIONS))
    st.session_state[key_agent] = agent
    return agent

# --------- UI ä¸»æµç¨‹ ----------
def load_langgraph_agenticai_app():
    st.title("ğŸ¤– LangChain Agentic AI")

    # åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ˆåªåšä¸€æ¬¡ï¼‰
    if "vector_index" not in st.session_state:
        with st.spinner("ğŸ“‚ æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°çŸ¥è¯†åº“ç´¢å¼•..."):
            try:
                st.session_state.vector_index = init_retrieval_pipeline()
                st.success("ğŸ“‚âœ… æœ¬åœ°çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
            except Exception as e:
                st.error(f"âŒ æœ¬åœ°çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
                return

    if "vector_index_urls" not in st.session_state:
        with st.spinner("ğŸŒ æ­£åœ¨åˆå§‹åŒ– URL çŸ¥è¯†åº“ç´¢å¼•..."):
            try:
                st.session_state.vector_index_urls = init_url_retrieval_pipeline()
                st.success("ğŸŒâœ… URL çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
            except Exception as e:
                st.error(f"âŒ URL çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("æ¨¡å‹é…ç½®")
        ui = LoadStreamlitUI(set_page_config=False)
        st.session_state.user_input = ui.load_streamlit_ui()

        st.header("å¯¹è¯ç®¡ç†")
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
            st.session_state.messages = []
            st.rerun()

    # æ˜¾ç¤ºå†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            placeholder = st.empty()
            try:
                user_input = st.session_state.get("user_input", {}) or {}
                selected_llm = user_input.get("selected_llm", "ollama")

                agent = _get_cached_agent(selected_llm)
                answer = asyncio.run(agent.ainvoke(prompt))

                placeholder.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                err = f"âŒ å‘ç”Ÿé”™è¯¯: {e}"
                logging.exception("assistant error")
                placeholder.markdown(err)
                st.session_state.messages.append({"role": "assistant", "content": err})









# import os
# import sys
# import asyncio
# import streamlit as st
# import logging
# from dotenv import load_dotenv
# os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")


# # ====== MCP & LangChain ======
# from langchain_mcp_adapters.client import MultiServerMCPClient
# from langchain.agents import initialize_agent, AgentType
# from langchain_community.chat_models import ChatOllama
# from langchain_openai import ChatOpenAI


# # ====== çŸ¥è¯†åº“ ======
# from langchainagenticai.utils.loaddocs import init_retrieval_pipeline
# from langchainagenticai.utils.loadurls import init_url_retrieval_pipeline
# from langchainagenticai.ui.streamlit.load_ui import LoadStreamlitUI
# from langchainagenticai.agent.agent import create_mcp_agent

# load_dotenv(override=True)

# # é¡¹ç›®æ ¹ç›®å½•åŠ å…¥è·¯å¾„
# project_root = os.path.abspath(os.path.join(os.getcwd(), "../../.."))
# sys.path.append(project_root)

# # æ—¥å¿—
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# # MCP å¤šæœåŠ¡å™¨å®¢æˆ·ç«¯
# MCP_CONNECTIONS = {
#     "LocalRetrievalQA": {
#         "command": "python",
#         "args": ["src/langchainagenticai/mcp_server/local_server.py"],
#         "transport": "stdio",
#     },
#     "CloudRetrievalQA": {
#         "command": "python",
#         "args": ["src/langchainagenticai/mcp_server/cloud_server.py"],
#         "transport": "stdio",
#     },
# }

# async def load_all_mcp_tools():
#     """ä»æ‰€æœ‰ MCP å®¢æˆ·ç«¯åŠ è½½å·¥å…·"""
#     tools = []
#     for client in mcp_clients.values():
#         client_tools = await client.get_tools()
#         tools.extend(client_tools)
#     return tools

# def load_langgraph_agenticai_app():
#     st.title("ğŸ¤– LangChain Agentic AI")

#     # åˆå§‹åŒ–çŸ¥è¯†åº“
#     if "vector_index" not in st.session_state:
#         with st.spinner("ğŸ“‚ æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°çŸ¥è¯†åº“ç´¢å¼•..."):
#             try:
#                 st.session_state.vector_index = init_retrieval_pipeline()
#                 st.success("ğŸ“‚âœ… æœ¬åœ°çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
#             except Exception as e:
#                 st.error(f"âŒ æœ¬åœ°çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
#                 return

#     if "vector_index_urls" not in st.session_state:
#         with st.spinner("ğŸŒ æ­£åœ¨åˆå§‹åŒ– URL çŸ¥è¯†åº“ç´¢å¼•..."):
#             try:
#                 st.session_state.vector_index_urls = init_url_retrieval_pipeline()
#                 st.success("ğŸŒâœ… URL çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
#             except Exception as e:
#                 st.error(f"âŒ URL çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")

#     # åˆå§‹åŒ–æ¶ˆæ¯å†å²
#     if "messages" not in st.session_state:
#         st.session_state.messages = []

#     # ä¾§è¾¹æ 
#     with st.sidebar:
#         st.header("æ¨¡å‹é…ç½®")
#         ui = LoadStreamlitUI(set_page_config=False)
#         st.session_state.user_input = ui.load_streamlit_ui()

#         st.header("å¯¹è¯ç®¡ç†")
#         if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
#             st.session_state.messages = []
#             st.rerun()

#     # æ˜¾ç¤ºå†å²æ¶ˆæ¯
#     for message in st.session_state.messages:
#         with st.chat_message(message["role"]):
#             st.markdown(message["content"])

#     # åœ¨æ”¶åˆ°ç”¨æˆ·è¾“å…¥æ—¶ï¼š
#     if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
#         st.session_state.messages.append({"role": "user", "content": prompt})
#         with st.chat_message("user"):
#             st.markdown(prompt)

#         with st.chat_message("assistant"):
#             message_placeholder = st.empty()

#             try:
#                 user_input = st.session_state.user_input
#                 selected_llm = user_input.get("selected_llm", "ollama")

#                 # 1) æ„å»ºæ¨¡å‹
#                 if selected_llm == "OpenAI":
#                     # â€”â€” åœ¨çº¿ï¼šOpenRouter â†’ openai/gpt-4o-mini ï¼ˆfunction callingï¼‰
#                     api_key = os.getenv("OPENROUTER_API_KEY")
#                     base_url = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
#                     model_name = os.getenv("OPENROUTER_MODEL", "openai/gpt-4o-mini")
#                     model = ChatOpenAI(model=model_name, api_key=api_key, base_url=base_url)
#                     # model = ChatOpenAI(model="openai/gpt-4o-mini")   # ä½ çš„å®é™…é€‰æ‹©
#                 else:
#                     model = ChatOllama(model="llama3.1:8b", base_url="http://localhost:11434")

#                 # 2) åˆ›å»º MCP Agentï¼ˆè¿æ¥æœ¬åœ°ä¸äº‘ç«¯ serverï¼ŒAgent è‡ªå·±å†³å®šç”¨å“ªä¸ªå·¥å…·ï¼‰
#                 agent = asyncio.run(create_mcp_agent(model, MCP_CONNECTIONS))

#                 # 3) äº¤ç»™ Agentï¼ˆå®ƒä¼š Thoughtâ†’Actionâ†’è°ƒç”¨ MCP toolâ†’Observationâ†’â€¦â†’Final Answerï¼‰
#                 answer = asyncio.run(agent.ainvoke(prompt))

#                 message_placeholder.markdown(answer)
#                 st.session_state.messages.append({"role": "assistant", "content": answer})

#             except Exception as e:
#                 error_msg = f"âŒ å‘ç”Ÿé”™è¯¯: {e}"
#                 message_placeholder.markdown(error_msg)
#                 st.session_state.messages.append({"role": "assistant", "content": error_msg})




# # src/langchainagenticai/main.py
# import os
# import sys
# import logging
# import streamlit as st

# # ä¿æŒä½ çš„ UI/æ£€ç´¢åˆå§‹åŒ–ä¸å˜
# from src.langchainagenticai.utils.loaddocs import init_retrieval_pipeline
# from src.langchainagenticai.utils.loadurls import init_url_retrieval_pipeline
# from src.langchainagenticai.ui.streamlit.load_ui import LoadStreamlitUI

# # æ–°çš„ Agentï¼ˆè´Ÿè´£è‡ªåŠ¨è·¯ç”± MCP å·¥å…·ï¼‰
# from src.langchainagenticai.agent.agent import MCPAgent 

# import logging
# import streamlit as st

# from src.langchainagenticai.utils.loaddocs import init_retrieval_pipeline
# from src.langchainagenticai.utils.loadurls import init_url_retrieval_pipeline
# from src.langchainagenticai.ui.streamlit.load_ui import LoadStreamlitUI
# from src.langchainagenticai.agent.agent import MCPAgent

# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


# def _init_states():
#     if "messages" not in st.session_state:
#         st.session_state.messages = []
#     if "vector_index" not in st.session_state:
#         with st.spinner("ğŸ“‚ æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°çŸ¥è¯†åº“ç´¢å¼•..."):
#             try:
#                 st.session_state.vector_index = init_retrieval_pipeline()
#                 st.success("ğŸ“‚âœ… æœ¬åœ°çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
#             except Exception as e:
#                 st.error(f"âŒ æœ¬åœ°çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
#     if "vector_index_urls" not in st.session_state:
#         with st.spinner("ğŸŒ æ­£åœ¨åˆå§‹åŒ– URL çŸ¥è¯†åº“ç´¢å¼•..."):
#             try:
#                 st.session_state.vector_index_urls = init_url_retrieval_pipeline()
#                 st.success("ğŸŒâœ… URL çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
#             except Exception as e:
#                 st.error(f"âŒ URL çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")


# def _render_sidebar():
#     st.sidebar.header("æ¨¡å‹é…ç½®")
#     ui = LoadStreamlitUI(set_page_config=False)
#     st.session_state.user_input = ui.load_streamlit_ui()

#     st.sidebar.header("å¯¹è¯ç®¡ç†")
#     user_messages = len([m for m in st.session_state.messages if m["role"] == "user"])
#     assistant_messages = len([m for m in st.session_state.messages if m["role"] == "assistant"])
#     st.sidebar.metric("ç”¨æˆ·æ¶ˆæ¯", user_messages)
#     st.sidebar.metric("åŠ©æ‰‹å›å¤", assistant_messages)

#     if st.sidebar.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
#         st.session_state.messages = []
#         st.rerun()

#     if st.session_state.messages:
#         if st.sidebar.button("ğŸ“¥ å¯¼å‡ºå¯¹è¯"):
#             conversation_text = ""
#             for msg in st.session_state.messages:
#                 role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
#                 conversation_text += f"{role}: {msg['content']}\n\n"

#             st.sidebar.download_button(
#                 label="ä¸‹è½½å¯¹è¯è®°å½•",
#                 data=conversation_text,
#                 file_name=f"conversation_{st.session_state.get('session_id', 'unknown')}.txt",
#                 mime="text/plain"
#             )


# def _render_history():
#     for message in st.session_state.messages:
#         with st.chat_message(message["role"]):
#             st.markdown(message["content"])


# def _handle_chat():
#     prompt = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")
#     if not prompt:
#         return

#     # å±•ç¤ºç”¨æˆ·æ¶ˆæ¯
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     # ç”ŸæˆåŠ©æ‰‹å›å¤
#     with st.chat_message("assistant"):
#         placeholder = st.empty()
#         try:
#             user_input = st.session_state.get("user_input", {})
#             selected_llm = (user_input.get("selected_llm", "ollama") or "ollama").lower()
#             backend = "openai" if selected_llm == "openai" else "ollama"

#             agent = MCPAgent(llm_backend=backend)
#             answer = agent.ask(prompt, history=st.session_state.messages)

#             placeholder.markdown(answer)
#             st.session_state.messages.append({"role": "assistant", "content": answer})

#         except Exception as e:
#             err = f"âŒ å‘ç”Ÿé”™è¯¯: {e}"
#             logging.exception("assistant error")
#             placeholder.markdown(err)
#             st.session_state.messages.append({"role": "assistant", "content": err})


# def main():
#     st.set_page_config(page_title="LangChain Agentic AI", page_icon="ğŸ¤–")
#     st.title("ğŸ¤– LangChain Agentic AI")
#     _init_states()
#     _render_sidebar()
#     _render_history()
#     _handle_chat()


# # å…¼å®¹è€å…¥å£ï¼ˆapp.py é‡Œå¯èƒ½è¿˜åœ¨ import è¿™ä¸ªï¼‰
# def load_langgraph_agenticai_app():
#     return main()


# __all__ = ["main", "load_langgraph_agenticai_app"]


# if __name__ == "__main__":
#     main()








# æ²¡é—®é¢˜
# import os
# import sys
# import asyncio
# import streamlit as st
# from pydantic import SecretStr
# from langchain_mcp_adapters.client import MultiServerMCPClient
# from langchainagenticai.utils.loaddocs import init_retrieval_pipeline
# from langchainagenticai.utils.loadurls import init_url_retrieval_pipeline
# from langchainagenticai.ui.streamlit.load_ui import LoadStreamlitUI
# from angchainagenticai.agent.agent import MCPAgent

# from langchainagenticai.ui.streamlit.display_result import DisplayResultStreamlit
# from langchain_openai import ChatOpenAI
# from langchainagenticai.mcp_server.local_server import local_answer_question
# from langchainagenticai.mcp_server.cloud_server import cloud_answer_question
# from pydantic import SecretStr
# import json
# import requests       
# from langchain_openai import OpenAI

# from langchain_community.chat_models import ChatOllama
# from langchain.chat_models import ChatOllama
# import logging

# # å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
# project_root = os.path.abspath(os.path.join(os.getcwd(), "../../.."))  # è·³å‡º src/langchainagenticai/retrieval
# sys.path.append(project_root)

# # é…ç½®æ—¥å¿—
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


# # MCP å®¢æˆ·ç«¯åˆå§‹åŒ–ï¼ˆå…¨å±€åªåˆ›å»ºä¸€æ¬¡ï¼‰
# mcp_clients = {
#     "CloudRetrievalQA": MultiServerMCPClient(
#         connections={
#             "CloudRetrievalQA": {
#                 "command": "python",
#                 "args": ["src/langchainagenticai/mcp_server/cloud_server.py"],
#                 "transport": "stdio",
#             }
#         }
#     ),
#     "LocalRetrievalQA": MultiServerMCPClient(
#         connections={
#             "LocalRetrievalQA": {
#                 "command": "python",
#                 "args": ["src/langchainagenticai/mcp_server/local_server.py"],
#                 "transport": "stdio",
#             }
#         }
#     ),
# }

# async def ask_cloud_tool(client, query):
#     tools = await client.get_tools()
#     cloud_tool = next((t for t in tools if t.name == "cloud_answer_question"), None)
#     if not cloud_tool:
#         return "âŒ æœªæ‰¾åˆ° cloud_answer_question å·¥å…·"
#     result = await cloud_tool.ainvoke({"query": query})
#     return result

# async def ask_local_tool(client, query):
#     tools = await client.get_tools()
#     local_tool = next((t for t in tools if t.name == "local_answer_question"), None)
#     if not local_tool:
#         return "âŒ æœªæ‰¾åˆ° local_answer_question å·¥å…·"
#     result = await local_tool.ainvoke({"query": query})
#     return result

# def load_langgraph_agenticai_app():
#     st.title("ğŸ¤– LangChain Agentic AI")

#     # åˆå§‹åŒ–çŸ¥è¯†åº“ç´¢å¼•ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
#     if "vector_index" not in st.session_state:
#         with st.spinner("ğŸ“‚ æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°çŸ¥è¯†åº“ç´¢å¼•..."):
#             try:
#                 st.session_state.vector_index = init_retrieval_pipeline()
#                 st.success("ğŸ“‚âœ… æœ¬åœ°çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
#             except Exception as e:
#                 st.error(f"âŒ æœ¬åœ°çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
#                 return

#     if "vector_index_urls" not in st.session_state:
#         with st.spinner("ğŸŒ æ­£åœ¨åˆå§‹åŒ– URL çŸ¥è¯†åº“ç´¢å¼•..."):
#             try:
#                 st.session_state.vector_index_urls = init_url_retrieval_pipeline()
#                 st.success("ğŸŒâœ… URL çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
#             except Exception as e:
#                 st.error(f"âŒ URL çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")

#     # åˆå§‹åŒ–æ¶ˆæ¯å†å²
#     if "messages" not in st.session_state:
#         st.session_state.messages = []

#     # ä¾§è¾¹æ 
#     with st.sidebar:
#         st.header("æ¨¡å‹é…ç½®")
        
#         # ä½¿ç”¨ LoadStreamlitUI æ¥æ˜¾ç¤ºå®Œæ•´çš„é…ç½®ç•Œé¢
#         ui = LoadStreamlitUI(set_page_config=False)
#         st.session_state.user_input = ui.load_streamlit_ui()

#         st.header("å¯¹è¯ç®¡ç†")
#         user_messages = len([msg for msg in st.session_state.messages if msg["role"] == "user"])
#         assistant_messages = len([msg for msg in st.session_state.messages if msg["role"] == "assistant"])
#         st.metric("ç”¨æˆ·æ¶ˆæ¯", user_messages)
#         st.metric("åŠ©æ‰‹å›å¤", assistant_messages)

#         if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
#             st.session_state.messages = []
#             st.rerun()
            
#         if st.session_state.messages:
#             if st.button("ğŸ“¥ å¯¼å‡ºå¯¹è¯"):
#                 conversation_text = ""
#                 for msg in st.session_state.messages:
#                     role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
#                     conversation_text += f"{role}: {msg['content']}\n\n"

#                 st.download_button(
#                     label="ä¸‹è½½å¯¹è¯è®°å½•",
#                     data=conversation_text,
#                     file_name=f"conversation_{st.session_state.get('session_id', 'unknown')}.txt",
#                     mime="text/plain"
#                 )

#     # æ˜¾ç¤ºå†å²æ¶ˆæ¯
#     for message in st.session_state.messages:
#         with st.chat_message(message["role"]):
#             st.markdown(message["content"])
    
#     if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
#         st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         message_placeholder = st.empty()
#         try:
#             user_input = st.session_state.user_input
#             selected_llm = (user_input.get("selected_llm", "ollama")).lower()

#             agent = MCPAgent(llm_backend=("openai" if selected_llm == "openai" else "ollama"))

#             # æŠŠå†å²ä¼ ç»™ Agentï¼ˆé€‰ï¼‰
#             answer = agent.ask(prompt, history=st.session_state.messages)

#             message_placeholder.markdown(answer)
#             st.session_state.messages.append({"role": "assistant", "content": answer})
#         except Exception as e:
#             err = f"âŒ å‘ç”Ÿé”™è¯¯: {e}"
#             message_placeholder.markdown(err)
#             st.session_state.messages.append({"role": "assistant", "content": err})

#     # ç”¨æˆ·è¾“å…¥çš„ æ²¡é—®é¢˜
#     # if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
#     #     st.session_state.messages.append({"role": "user", "content": prompt})
#     #     with st.chat_message("user"):
#     #         st.markdown(prompt)

#     #     with st.chat_message("assistant"):
#     #         message_placeholder = st.empty()

#     #         try:
#     #             user_input = st.session_state.user_input
#     #             selected_llm = user_input.get("selected_llm", "ollama")

#     #             # ç”±äº Streamlit æœ¬èº«æ˜¯åŒæ­¥è¿è¡Œï¼Œè°ƒç”¨å¼‚æ­¥è¦ç”¨ asyncio.run æˆ– asyncio.create_task
#     #             # ä½† asyncio.run åœ¨ Streamlit é‡Œå¯èƒ½æœ‰äº‹ä»¶å¾ªç¯å†²çªï¼Œæ¨èä½¿ç”¨ asyncio.run if no running loop
#     #             # è¿™é‡Œåšç®€å•å¤„ç†ï¼š

#     #             if selected_llm == "OpenAI":
#     #                 client = mcp_clients["CloudRetrievalQA"]
#     #                 answer = asyncio.run(ask_cloud_tool(client, prompt))
#     #             else:
#     #                 client = mcp_clients["LocalRetrievalQA"]
#     #                 answer = asyncio.run(ask_local_tool(client, prompt))

#     #             message_placeholder.markdown(answer)
#     #             st.session_state.messages.append({"role": "assistant", "content": answer})

#     #         except Exception as e:
#     #             error_msg = f"âŒ å‘ç”Ÿé”™è¯¯: {e}"
#     #             logging.error(error_msg)
#     #             message_placeholder.markdown(error_msg)
#     #             st.session_state.messages.append({"role": "assistant", "content": error_msg})

    