import streamlit as st
from langchainagenticai.ui.streamlit.load_ui import LoadStreamlitUI
from langchainagenticai.ui.streamlit.display_result import DisplayResultStreamlit
from langchain_openai import ChatOpenAI
from langchainagenticai.mcp_server.local_server import local_answer_question
import asyncio
import os
from pydantic import SecretStr
import json
import requests
import sys

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
project_root = os.path.abspath(os.path.join(os.getcwd(), "../../.."))  # è·³å‡º src/langchainagenticai/retrieval
sys.path.append(project_root)


def load_langgraph_agenticai_app():
    st.title("LangChain Agentic AI")
    
    # åˆå§‹åŒ–æ¶ˆæ¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ä¾§è¾¹æ åŠŸèƒ½
    with st.sidebar:
        st.header("æ¨¡å‹é…ç½®")
        
        # åŠ è½½UIé…ç½®
        ui = LoadStreamlitUI()
        user_input = ui.load_streamlit_ui()
        
        st.header("å¯¹è¯ç®¡ç†")
        
        # æ˜¾ç¤ºæ¶ˆæ¯ç»Ÿè®¡
        user_messages = len([msg for msg in st.session_state.messages if msg["role"] == "user"])
        assistant_messages = len([msg for msg in st.session_state.messages if msg["role"] == "assistant"])
        st.metric("ç”¨æˆ·æ¶ˆæ¯", user_messages)
        st.metric("åŠ©æ‰‹å›å¤", assistant_messages)
        
        # æ¸…é™¤å†å²æŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
            st.session_state.messages = []
            st.rerun()
        
        # å¯¼å‡ºå¯¹è¯åŠŸèƒ½
        if st.session_state.messages:
            if st.button("ğŸ“¥ å¯¼å‡ºå¯¹è¯"):
                # ç”Ÿæˆå¯¹è¯æ–‡æœ¬
                conversation_text = ""
                for msg in st.session_state.messages:
                    role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
                    conversation_text += f"{role}: {msg['content']}\n\n"
                
                # æä¾›ä¸‹è½½
                st.download_button(
                    label="ä¸‹è½½å¯¹è¯è®°å½•",
                    data=conversation_text,
                    file_name=f"conversation_{st.session_state.get('session_id', 'unknown')}.txt",
                    mime="text/plain"
                )
    
    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            try:
                # è·å–é€‰æ‹©çš„æ¨¡å‹ä¿¡æ¯
                selected_llm = user_input.get("selected_llm")
                selected_model = user_input.get("selected_model")
                
                if selected_llm == "OpenAI":
                    api_key = user_input.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY") or os.getenv("OPENROUTER_API_KEY")
                    if not api_key:
                        st.error("âš ï¸ è¯·æä¾› OpenAI API Key")
                        return
                    
                    model = ChatOpenAI(
                        model=selected_model,
                        base_url="https://openrouter.ai/api/v1",
                        api_key=SecretStr(api_key),
                        max_tokens=4000,  # é™åˆ¶æœ€å¤§ token æ•°é‡
                        temperature=0.7,
                    )
                    
                    response = model.invoke(prompt)
                    response_content = getattr(response, "content", response)
                    message_placeholder.markdown(response_content)
                    
                    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                    st.session_state.messages.append({"role": "assistant", "content": response_content})
                    
                elif selected_llm == "ollama":
                    answer = asyncio.run(local_answer_question(prompt))
                    message_placeholder.markdown(answer)
                    
                    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                    
                else:
                    st.error(f"âš ï¸ ä¸æ”¯æŒçš„LLMç±»å‹: {selected_llm}")
                    
            except Exception as e:
                error_message = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
                message_placeholder.markdown(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})
