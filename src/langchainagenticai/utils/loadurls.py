import os
import logging
from dotenv import load_dotenv
import json

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åŠ è½½ .env
load_dotenv()

def load_urls_from_env():
    """ä» .env æ–‡ä»¶è¯»å– CLOUD_URLS"""
    urls_str = os.getenv("CLOUD_URLS", "[]")
    try:
        urls = json.loads(urls_str) if urls_str.startswith("[") else [u.strip() for u in urls_str.split(",") if u.strip()]
        logger.info(f"ğŸŒ ä»ç¯å¢ƒå˜é‡åŠ è½½ {len(urls)} ä¸ª URL")
        return urls
    except Exception as e:
        logger.error(f"âŒ è§£æ CLOUD_URLS å¤±è´¥: {e}")
        return []


def fetch_url_content(urls):
    """æŠ“å–ç½‘é¡µå†…å®¹"""
    all_documents = []
    for url in urls:
        try:
            logger.info(f"ğŸ” æŠ“å– URL: {url}")
            loader = WebBaseLoader(url)
            docs = loader.load()
            all_documents.extend(docs)
        except Exception as e:
            logger.error(f"âŒ æŠ“å– {url} å¤±è´¥: {e}")
    return all_documents


def split_documents(documents, chunk_size=800, overlap=120):
    """å¯¹ç½‘é¡µæ–‡æ¡£è¿›è¡Œåˆ†ç‰‡"""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    return text_splitter.split_documents(documents)


def build_index(documents):
    """å‘é‡åŒ–å¹¶æ„å»ºä¸´æ—¶ç´¢å¼•ï¼ˆå†…å­˜æ•°æ®åº“ï¼‰"""
    embeddings = SentenceTransformerEmbeddings(model_name="shibing624/text2vec-base-chinese")
    doc_objects = [Document(page_content=str(doc.page_content), metadata=doc.metadata) for doc in documents]
    index = Chroma.from_documents(
        documents=doc_objects,
        embedding=embeddings,
        collection_name="cloud_urls",
        persist_directory=None
    )
    logger.info(f"âœ… URL å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆï¼Œå…± {index._collection.count()} ä¸ªåˆ†ç‰‡")
    return index


def init_url_retrieval_pipeline():
    """åˆå§‹åŒ– URL æ£€ç´¢æµç¨‹ï¼ˆå¯åœ¨ app.py å¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
    logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– URL æ£€ç´¢ç³»ç»Ÿ...")
    urls = load_urls_from_env()
    if not urls:
        logger.warning("âš ï¸ æœªé…ç½®ä»»ä½• CLOUD_URLS")
        return None

    raw_docs = fetch_url_content(urls)
    if not raw_docs:
        logger.warning("âš ï¸ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•ç½‘é¡µå†…å®¹")
        return None

    split_docs = split_documents(raw_docs)
    index = build_index(split_docs)
    logger.info(f"âœ… URL æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œåˆ†ç‰‡æ•°: {len(split_docs)}")
    return index