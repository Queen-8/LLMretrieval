# loaddocs.pyä»£ç å¦‚ä¸‹
import logging
import os
import glob
from dotenv import load_dotenv  # ç”¨äºåŠ è½½.envç¯å¢ƒå˜é‡
from langchain.schema import Document
from langchain_community.document_loaders import PyMuPDFLoader

# from langchain.text_splitter import CharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

from sentence_transformers import SentenceTransformer
# from sentence_transformers import CrossEncoder
from sentence_transformers.cross_encoder import CrossEncoder
from langchain.chat_models import ChatOllama

# å»ºç«‹ç´¢å¼•å¹¶å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.schema import Document

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åŠ è½½ .env ä¸­çš„é…ç½®
load_dotenv()

folder_path = os.getenv("LOCAL_KNOWLEDGE_BASE_PATH", "Local_knowledge_base")
print("ğŸ“‚ åŠ è½½è·¯å¾„:", folder_path)

# åŠ è½½ .env ä¸­çš„é…ç½®
load_dotenv()
folder_path = os.getenv("LOCAL_KNOWLEDGE_BASE_PATH", "Local_knowledge_base")
print("ğŸ“‚ åŠ è½½è·¯å¾„:", folder_path)


# 1. åŠ è½½ PDF æ–‡æ¡£
def load_pdf_from_folder(folder_path):
    all_documents = []
    pdf_files = glob.glob(f"{folder_path}/*.pdf")
    print(f"ğŸ“„ åŒ¹é…åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶ï¼š", pdf_files)

    for filename in pdf_files:
        loader = PyMuPDFLoader(filename)
        documents = loader.load()
        for doc in documents:
            # åªä¿ç•™ source ä¿¡æ¯ï¼Œä¸æ·»åŠ  private æ ‡è®°
            doc.metadata["source"] = filename
        all_documents.extend(documents)

    return all_documents


# 2. æ–‡æœ¬åˆ†ç‰‡
def split_documents(documents, chunk_size=1000, overlap=100):
    """
    å¯¹æ¯ä¸ªæ–‡æ¡£åšåˆ†ç‰‡ï¼Œchunk_size æ§åˆ¶å•ç‰‡é•¿åº¦ï¼Œoverlap ä¿è¯ä¸Šä¸‹æ–‡è¿è´¯
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    return text_splitter.split_documents(documents)



# 3. å»ºç«‹å‘é‡ç´¢å¼•ï¼ˆå†…å­˜æ¨¡å¼ï¼‰
def build_index(documents):
    embeddings = SentenceTransformerEmbeddings(model_name="shibing624/text2vec-base-chinese")
    index = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        collection_name="default",
        persist_directory=None
    )
    logger.info(f"âœ… å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {index._collection.count()} ä¸ªæ–‡æ¡£")
    return index


# 4. åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ
def init_retrieval_pipeline():
    logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ...")
    raw_documents = load_pdf_from_folder(folder_path)
    split_docs = split_documents(raw_documents)
    index = build_index(split_docs)
    logger.info(f"âœ… æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {len(split_docs)} ä¸ªæ–‡æ¡£åˆ†ç‰‡")
    return index