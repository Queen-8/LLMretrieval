# local_retrieval.py ä»£ç å¦‚ä¸‹
import sys  # ç¡®ä¿å¯¼å…¥ sys æ¨¡å—
import os
import glob
import logging
from ollama import Client
# from langchainagenticai.utils.base_retrieval import split_documents, build_index, recall_documents, rerank_documents
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chat_models import ChatOllama
from sentence_transformers import CrossEncoder
# from langchain_ollama import ChatOllama
from langchain_community.chat_models import ChatOllama
from langchain.chat_models import ChatOllama
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
project_root = os.path.abspath(os.path.join(os.getcwd(), "../../.."))  # è·³å‡º src/langchainAgenticAi/retrieval
sys.path.append(project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# å…¨å±€ç¼“å­˜
index = None


from src.langchainagenticai.utils.loaddocs import init_retrieval_pipeline  # ç”¨loaddocs.py



def get_index():
    global index
    if index is None:
        index = init_retrieval_pipeline()
    return index

def recall_documents(query: str, k: int = 5):
    """
    ç›´æ¥åŸºäºç›¸ä¼¼åº¦æ£€ç´¢ï¼Œè¿”å›å‰ k ç¯‡æœ€ç›¸ä¼¼æ–‡æ¡£
    """
    index = get_index()
    results = index.similarity_search(query, k=k)

    logger.info(f"ğŸ” æ£€ç´¢åˆ° {len(results)} ç¯‡æ–‡æ¡£ï¼ˆTop {k}ï¼‰ï¼š")
    for i, doc in enumerate(results, 1):
        logger.info(f"{i}. {doc.page_content[:50]}... æ¥æº: {doc.metadata.get('source')}")
    return results


def rerank(query: str, retrieved_docs, top_k: int):
    """
    ä½¿ç”¨ CrossEncoder å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’
    """
    retrieved_chunks = [doc.page_content for doc in retrieved_docs]
    pairs = [(query, chunk) for chunk in retrieved_chunks]

    cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')
    scores = cross_encoder.predict(pairs)

    scored_chunks = list(zip(retrieved_chunks, scores))
    scored_chunks.sort(key=lambda x: x[1], reverse=True)

    logger.info(f"ğŸ“Š é‡æ’åé€‰å–å‰ {top_k} æ®µ")
    return [chunk for chunk, _ in scored_chunks][:top_k]


def generate_answer(query, reranked_chunks):
    """
    åŸºäºæŸ¥è¯¢å’Œé‡æ’åçš„ chunk æ–‡æœ¬ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    """
    documents_content = "\n".join(reranked_chunks)

    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n"
        f"{documents_content}\n\nç”¨æˆ·æé—®ï¼š{query}\n\nè¯·ç”¨ç®€æ´å‡†ç¡®çš„è¯­è¨€ä½œç­”ï¼š"
    )

    llm = ChatOllama(base_url="http://localhost:11434", model="llama3.1:8b")
    print(f"DEBUG: llm instance = {llm}")

    try:
        response = llm.invoke(prompt)
        print(f"DEBUG: raw response = {response}; type = {type(response)}")

        if isinstance(response, str):
            return response.strip()
        elif isinstance(response, list) and len(response) > 0:
            first = response[0]
            if isinstance(first, str):
                return first.strip()
            elif isinstance(first, dict):
                content = first.get("content") or first.get("text") or first.get("answer")
                if isinstance(content, str):
                    return content.strip()
                else:
                    return str(first).strip()
            else:
                return str(first).strip()
        elif hasattr(response, "content") and isinstance(response.content, str):
            return response.content.strip()
        elif hasattr(response, "text") and isinstance(response.text, str):
            return response.text.strip()
        else:
            return str(response).strip()

    except Exception as e:
        print(f"âŒ Error generating answer: {e}")
        return None



# def recall_documents(query : str, index, k=5):
#     """
#     A similarity retrieval is performed and the k most similar documents to the query are returned
#     """
#     return index.similarity_search(query, k=k)

# def rerank(query, retrieved_docs, top_k: int):
#     """
#     Rerank the retrieved document chunks using a cross-encoder model and return the top_k contents
#     """
#     # æå–æ¯ä¸ª Document çš„å†…å®¹
#     retrieved_chunks = [doc.page_content for doc in retrieved_docs]
    
#     # åˆ›å»º query-chunk å¯¹
#     pairs = [(query, chunk) for chunk in retrieved_chunks]

#     # ä½¿ç”¨ CrossEncoder è¿›è¡Œæ‰“åˆ†
#     cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')
#     scores = cross_encoder.predict(pairs)

#     # å°† chunk ä¸åˆ†æ•°ç»„æˆå…ƒç»„ï¼Œå¹¶æŒ‰åˆ†æ•°é™åºæ’åº
#     scored_chunks = list(zip(retrieved_chunks, scores))
#     scored_chunks.sort(key=lambda x: x[1], reverse=True)

#     # è¿”å› Top-K é‡æ’åçš„ chunk æ–‡æœ¬
#     return [chunk for chunk, _ in scored_chunks][:top_k]


# def generate_answer(query, reranked_chunks):
#     """
#     åŸºäºæŸ¥è¯¢å’Œé‡æ’åçš„ chunk æ–‡æœ¬ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
#     """
#     documents_content = "\n".join(reranked_chunks)

#     prompt = (
#         "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n"
#         f"{documents_content}\n\nç”¨æˆ·æé—®ï¼š{query}\n\nè¯·ç”¨ç®€æ´å‡†ç¡®çš„è¯­è¨€ä½œç­”ï¼š"
#     )

#     llm = ChatOllama(base_url="http://localhost:11434", model="llama3.1:8b")
#     print(llm)

#     try:
#         response = llm.invoke(prompt)
#         return response.content.strip()
#     except Exception as e:
#         print(f"âŒ Error generating answer: {e}")
#         return None