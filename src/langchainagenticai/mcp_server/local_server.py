# local_server.py
import os
os.environ.setdefault("USER_AGENT", "LangChain-AgenticAI/1.0 (contact: 564752114@qq.com)")

from mcp.server.fastmcp import FastMCP
import logging
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from langchain_huggingface import HuggingFaceEmbeddings
from src.langchainagenticai.retrieval.local_retrieval import recall_documents, rerank, generate_answer
from src.langchainagenticai.utils.loaddocs import init_retrieval_pipeline

logging.basicConfig(level=logging.INFO)

# åˆ›å»º FastMCP å®ä¾‹
mcp = FastMCP("LocalRetrievalQA")

# å…¨å±€å˜é‡
conversation_history = []
index = None
embedding_model = None  # ç¡®ä¿å…¨å±€ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹



@mcp.tool(
    name="local_answer_question", 
    description=(
        "ç”¨é€”ï¼šå›ç­”ä¸æœ¬åœ°å­˜å‚¨çš„ PDF æ–‡æ¡£ã€å†…éƒ¨èµ„æ–™ã€ç¦»çº¿æ–‡ä»¶ç›¸å…³çš„é—®é¢˜ã€‚"
        "é€‚åˆåœºæ™¯ï¼šå½“ç”¨æˆ·çš„é—®é¢˜æ¶‰åŠæœ¬åœ°æ–‡ä»¶ã€å…¬å¸å†…éƒ¨æ‰‹å†Œã€é¡¹ç›®æ–‡æ¡£ã€åŸ¹è®­èµ„æ–™ç­‰ã€‚"
        "è¾“å…¥ï¼šJSON æ ¼å¼ï¼ŒåŒ…å« query å­—æ®µï¼Œä¾‹å¦‚ï¼š{{\"query\": \"ç”¨æˆ·é—®é¢˜\"}}"
        "é™åˆ¶ï¼šä¸é€‚åˆå›ç­”å®æ—¶äº’è”ç½‘æ•°æ®çš„é—®é¢˜ã€‚"
    )
)
async def local_answer_question(input_data: dict) -> str:
    """
    ä»æœ¬åœ° PDF æ–‡æ¡£ä¸­æ£€ç´¢å†…å®¹å¹¶å›ç­”ç”¨æˆ·é—®é¢˜
    """
    try:
        # å¤„ç†è¾“å…¥å‚æ•°
        if isinstance(input_data, dict) and "query" in input_data:
            query = input_data["query"]
        else:
            return "âŒ å‚æ•°æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åŒ…å« query å­—æ®µçš„å­—å…¸"
        
        logging.info(f"ğŸ”§ æ”¶åˆ°æŸ¥è¯¢: {query}")
        
        # æ­¥éª¤1ï¼šå¬å›ç›¸å…³æ–‡æ¡£
        logging.info("ğŸ”§ å¼€å§‹å¬å›æ–‡æ¡£...")
        retrieved_docs = recall_documents(query, k=10)

        if not retrieved_docs:
            return "æœªèƒ½ä»æœ¬åœ°æ–‡æ¡£ä¸­æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # æ­¥éª¤2ï¼šé‡æ’ Top-k æ–‡æ¡£ç‰‡æ®µ
        logging.info("ğŸ”§ å¼€å§‹é‡æ’æ–‡æ¡£...")
        reranked_chunks = rerank(query, retrieved_docs, top_k=3)

        if not reranked_chunks:
            return "æ£€ç´¢åˆ°äº†å†…å®¹ï¼Œä½†æ— æ³•è¯†åˆ«æœ€ç›¸å…³ç‰‡æ®µã€‚"

        # æ­¥éª¤3ï¼šç”Ÿæˆç­”æ¡ˆ
        logging.info("ğŸ”§ è°ƒç”¨ Ollama å¼€å§‹ç”Ÿæˆç­”æ¡ˆ...")
        answer = generate_answer(query, reranked_chunks)

        return answer or "ç”Ÿæˆç­”æ¡ˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"

    except Exception as e:
        return f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}"


if __name__ == "__main__":
    logging.info("ğŸš€ å¯åŠ¨æœ¬åœ° MCP æœåŠ¡")
    mcp.run(transport="stdio")
