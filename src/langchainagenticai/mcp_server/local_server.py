from mcp.server.fastmcp import FastMCP
import logging
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from langchain_huggingface import HuggingFaceEmbeddings
from src.langchainagenticai.retrieval.local_retrieval import recall_documents, rerank, generate_answer
from src.langchainagenticai.utils.loaddocs import init_retrieval_pipeline

logging.basicConfig(level=logging.INFO)

# åˆ›å»º FastMCP å®ä¾‹
mcp = FastMCP("LocalRetrievalQA")

# å…¨å±€å˜é‡
conversation_history = []
index = None
embedding_model = None  # ç¡®ä¿å…¨å±€ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹


# @mcp.tool(name="local_answer_question", description="Answer questions from users based on local PDF documents")
# async def local_answer_question(query: str, filter_private: bool = True) -> str:
#     """
#     ä»æœ¬åœ° PDF æ–‡æ¡£ä¸­æ£€ç´¢å†…å®¹å¹¶å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
#     """
#     global index, conversation_history, embedding_model

#     try:
#         logging.info(f"ğŸ”§ æ”¶åˆ°æŸ¥è¯¢: {query}")

#         # å¦‚æœç´¢å¼•æœªåˆå§‹åŒ–ï¼Œå…ˆåˆå§‹åŒ–
#         if index is None:
#             logging.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ...")
#             embedding_model = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")
#             index = init_retrieval_pipeline(embedding=embedding_model)
#             logging.info("âœ… æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

#             # ç»Ÿè®¡æ–‡æ¡£ä¿¡æ¯
#             all_docs = list(index.docstore._dict.values())
#             private_count = sum(1 for d in all_docs if d.metadata.get("private", False))
#             logging.info(f"ğŸ“Š ç´¢å¼•æ–‡æ¡£æ€»æ•°: {len(all_docs)}, å…¶ä¸­ private æ–‡æ¡£æ•°: {private_count}")

#         # è¿‡æ»¤å‡½æ•°
#         def filter_func(doc):
#             if filter_private:
#                 return not (doc.metadata and doc.metadata.get("private", False))
#             return True

#         # å¬å›æ—¶ä»…ç”¨ç”¨æˆ· queryï¼Œä¿è¯è¯­ä¹‰çº¯å‡€
#         logging.info("ğŸ”§ å¼€å§‹å¬å›æ–‡æ¡£...")
#         retrieved_docs = recall_documents(query, index, k=10, filter_func=filter_func)

#         if not retrieved_docs:
#             logging.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
#             return "æœªèƒ½ä»æœ¬åœ°æ–‡æ¡£ä¸­æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

#         # å°†å†å²å¯¹è¯æ‹¼æ¥åˆ°ç”Ÿæˆé˜¶æ®µ
#         context_prompt = ""
#         for q, a in conversation_history:
#             context_prompt += f"ç”¨æˆ·: {q}\nåŠ©æ‰‹: {a}\n"
#         context_prompt += f"ç”¨æˆ·: {query}\nåŠ©æ‰‹:"

#         # é‡æ’
#         logging.info("ğŸ”§ å¼€å§‹é‡æ’æ–‡æ¡£...")
#         reranked_chunks = rerank(context_prompt, retrieved_docs, top_k=3)

#         if not reranked_chunks:
#             logging.warning("âš ï¸ é‡æ’åæ— ç›¸å…³ç‰‡æ®µ")
#             return "æ£€ç´¢åˆ°äº†å†…å®¹ï¼Œä½†æ— æ³•è¯†åˆ«æœ€ç›¸å…³ç‰‡æ®µã€‚"

#         # ç”Ÿæˆç­”æ¡ˆ
#         logging.info("ğŸ”§ å¼€å§‹ç”Ÿæˆç­”æ¡ˆ...")
#         answer = generate_answer(context_prompt, reranked_chunks)

#         # ç¼“å­˜å¯¹è¯
#         conversation_history.append((query, answer))
        
#         logging.info(f"âœ… ç”Ÿæˆç­”æ¡ˆå®Œæˆ: {answer[:100]}...")
#         return answer or "ç”Ÿæˆç­”æ¡ˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"

#     except Exception as e:
#         logging.error(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
#         return f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}"


# if __name__ == "__main__":
#     logging.info("ğŸš€ å¯åŠ¨æœ¬åœ° MCP æœåŠ¡")
#     mcp.run(transport="stdio")


#  æ²¡é—®é¢˜
@mcp.tool(name="local_answer_question", description="Answer questions from users based on local PDF documents")
async def local_answer_question(query: str) -> str:
    """
    ä»æœ¬åœ° PDF æ–‡æ¡£ä¸­æ£€ç´¢å†…å®¹å¹¶å›ç­”ç”¨æˆ·é—®é¢˜
    """
    try:

        logging.info(f"ğŸ”§ æ”¶åˆ°æŸ¥è¯¢: {query}")
        
        # æ­¥éª¤1ï¼šå¬å›ç›¸å…³æ–‡æ¡£
        logging.info("ğŸ”§ å¼€å§‹å¬å›æ–‡æ¡£...")
        retrieved_docs = recall_documents(query, k=10)

        if not retrieved_docs:
            return "æœªèƒ½ä»æœ¬åœ°æ–‡æ¡£ä¸­æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # æ­¥éª¤2ï¼šé‡æ’ Top-k æ–‡æ¡£ç‰‡æ®µ
        logging.info("ğŸ”§ å¼€å§‹é‡æ’æ–‡æ¡£...")
        reranked_chunks = rerank(query, retrieved_docs, top_k=3)

        if not reranked_chunks:
            return "æ£€ç´¢åˆ°äº†å†…å®¹ï¼Œä½†æ— æ³•è¯†åˆ«æœ€ç›¸å…³ç‰‡æ®µã€‚"

        # æ­¥éª¤3ï¼šç”Ÿæˆç­”æ¡ˆ
        logging.info("ğŸ”§ è°ƒç”¨ Ollama å¼€å§‹ç”Ÿæˆç­”æ¡ˆ...")
        answer = generate_answer(query, reranked_chunks)

        return answer or "ç”Ÿæˆç­”æ¡ˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"

    except Exception as e:
        return f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}"


if __name__ == "__main__":
    logging.info("ğŸš€ å¯åŠ¨æœ¬åœ° MCP æœåŠ¡")
    mcp.run(transport="stdio")
