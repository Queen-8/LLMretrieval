# cloud_server.py
from mcp.server.fastmcp import FastMCP
import logging
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.langchainagenticai.retrieval.cloud_retrieval import recall_documents, rerank, generate_answer_openai
from src.langchainagenticai.utils.loadurls import init_url_retrieval_pipeline

logging.basicConfig(level=logging.INFO)

# åˆ›å»º FastMCP å®ä¾‹
mcp = FastMCP("CloudRetrievalQA")

# å…¨å±€å˜é‡
conversation_history = []
index = None


@mcp.tool(name="cloud_answer_question", description="Answer questions from cloud URL documents")
async def cloud_answer_question(query: str) -> str:
    """
    ä»äº‘ç«¯ URL æ–‡æ¡£ä¸­æ£€ç´¢å†…å®¹å¹¶å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
    """
    global index, conversation_history

    try:
        logging.info(f"ğŸŒ æ”¶åˆ°äº‘ç«¯æŸ¥è¯¢: {query}")

        # å¦‚æœç´¢å¼•æœªåˆå§‹åŒ–ï¼Œå…ˆåˆå§‹åŒ–
        if index is None:
            logging.info("ğŸŒ æ­£åœ¨åˆå§‹åŒ–äº‘ç«¯æ£€ç´¢ç³»ç»Ÿ...")
            index = init_url_retrieval_pipeline()
            if index is None:
                return "âš ï¸ äº‘ç«¯æ–‡æ¡£ç´¢å¼•æœªèƒ½åˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ CLOUD_URLS é…ç½®"
            logging.info("âœ… äº‘ç«¯æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

        # æ„é€ ä¸Šä¸‹æ–‡
        context_prompt = ""
        for q, a in conversation_history:
            context_prompt += f"ç”¨æˆ·: {q}\nåŠ©æ‰‹: {a}\n"
        context_prompt += f"ç”¨æˆ·: {query}\nåŠ©æ‰‹:"

        # Step1: å¬å›
        logging.info("ğŸŒ å¼€å§‹å¬å›äº‘ç«¯æ–‡æ¡£...")
        retrieved_docs = recall_documents(context_prompt, index, k=10)

        if not retrieved_docs:
            logging.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³äº‘ç«¯æ–‡æ¡£")
            return "æœªèƒ½ä»äº‘ç«¯æ–‡æ¡£ä¸­æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # Step2: é‡æ’
        logging.info("ğŸŒ å¼€å§‹é‡æ’äº‘ç«¯æ–‡æ¡£...")
        reranked_chunks = rerank(context_prompt, retrieved_docs, top_k=3)

        if not reranked_chunks:
            logging.warning("âš ï¸ é‡æ’åæ— ç›¸å…³äº‘ç«¯ç‰‡æ®µ")
            return "æ£€ç´¢åˆ°äº†å†…å®¹ï¼Œä½†æ— æ³•è¯†åˆ«æœ€ç›¸å…³ç‰‡æ®µã€‚"

        # Step3: ç”Ÿæˆç­”æ¡ˆ
        logging.info("ğŸŒ è°ƒç”¨ OpenRouter ç”Ÿæˆç­”æ¡ˆ...")
        api_key = os.getenv("OPENAI_API_KEY") or os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            return "âš ï¸ æœªåœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® OPENAI_API_KEY æˆ– OPENROUTER_API_KEY"

        answer = generate_answer_openai(context_prompt, reranked_chunks, api_key=api_key)
        
        if answer is None:
            return "âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API key æ˜¯å¦æ­£ç¡®æˆ–ç¨åå†è¯•"

        # ç¼“å­˜å¯¹è¯
        conversation_history.append((query, answer))

        logging.info(f"âœ… äº‘ç«¯ç­”æ¡ˆç”Ÿæˆå®Œæˆ: {answer[:100]}...")
        return answer

    except Exception as e:
        logging.error(f"âŒ å¤„ç†äº‘ç«¯é—®é¢˜æ—¶å‡ºé”™: {e}")
        return f"âŒ å¤„ç†äº‘ç«¯é—®é¢˜æ—¶å‡ºé”™: {e}"


if __name__ == "__main__":
    logging.info("ğŸš€ å¯åŠ¨äº‘ç«¯ MCP æœåŠ¡")
    mcp.run(transport="stdio")
