# cloud_server.py 
import os
os.environ.setdefault("USER_AGENT", "LangChain-AgenticAI/1.0 (contact: 564752114@qq.com)")

from mcp.server.fastmcp import FastMCP
import logging
import os
import sys
from pathlib import Path
from langchain_openai import ChatOpenAI

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.langchainagenticai.retrieval.cloud_retrieval import recall_documents, rerank, generate_answer_openai
from src.langchainagenticai.utils.loadurls import init_url_retrieval_pipeline

logging.basicConfig(level=logging.INFO)

# åˆ›å»º FastMCP å®ä¾‹
mcp = FastMCP("CloudRetrievalQA")

# å…¨å±€å˜é‡
conversation_history = []
index = None


@mcp.tool(
    name="cloud_answer_question", 
    description=(
        "ç”¨é€”ï¼šå›ç­”ä¸äº‘ç«¯ URL æ–‡æ¡£ã€ç½‘é¡µæ–‡ç« ã€å…¬å¸åœ¨çº¿çŸ¥è¯†åº“ã€äº’è”ç½‘æ•°æ®ç›¸å…³çš„é—®é¢˜ã€‚"
        "é€‚åˆåœºæ™¯ï¼šå½“ç”¨æˆ·çš„é—®é¢˜æ¶‰åŠåœ¨çº¿å†…å®¹ã€å¤–éƒ¨æ•°æ®ã€ç½‘é¡µæ–‡ç« ã€ç½‘ç»œæ–°é—»ç­‰ã€‚"
        "è¾“å…¥ï¼šJSON æ ¼å¼ï¼ŒåŒ…å« query å­—æ®µï¼Œä¾‹å¦‚ï¼š{{\"query\": \"ç”¨æˆ·é—®é¢˜\"}}"
        "é™åˆ¶ï¼šå¦‚æœé—®é¢˜æ˜ç¡®ä¸æœ¬åœ°èµ„æ–™ç›¸å…³ï¼Œè¯·ä¸è¦ä½¿ç”¨æœ¬å·¥å…·ã€‚"
    )
)
async def cloud_answer_question(input_data: dict) -> str:
    """
    ä»äº‘ç«¯ URL æ–‡æ¡£ä¸­æ£€ç´¢å†…å®¹å¹¶å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
    """
    global index, conversation_history

    try:
        # å¤„ç†è¾“å…¥å‚æ•°
        if isinstance(input_data, dict) and "query" in input_data:
            query = input_data["query"]
        else:
            return "âŒ å‚æ•°æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åŒ…å« query å­—æ®µçš„å­—å…¸"
        
        logging.info(f"ğŸŒ æ”¶åˆ°äº‘ç«¯æŸ¥è¯¢: {query}")

        # å¦‚æœç´¢å¼•æœªåˆå§‹åŒ–ï¼Œå…ˆåˆå§‹åŒ–
        if index is None:
            logging.info("ğŸŒ æ­£åœ¨åˆå§‹åŒ–äº‘ç«¯æ£€ç´¢ç³»ç»Ÿ...")
            index = init_url_retrieval_pipeline()
            if index is None:
                return "âš ï¸ äº‘ç«¯æ–‡æ¡£ç´¢å¼•æœªèƒ½åˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ CLOUD_URLS é…ç½®"
            logging.info("âœ… äº‘ç«¯æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

        # æ„é€ ä¸Šä¸‹æ–‡
        context_prompt = ""
        for q, a in conversation_history:
            context_prompt += f"ç”¨æˆ·: {q}\nåŠ©æ‰‹: {a}\n"
        context_prompt += f"ç”¨æˆ·: {query}\nåŠ©æ‰‹:"

        # Step1: å¬å›
        logging.info("ğŸŒ å¼€å§‹å¬å›äº‘ç«¯æ–‡æ¡£...")
        retrieved_docs = recall_documents(context_prompt, index, k=10)

        if not retrieved_docs:
            logging.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³äº‘ç«¯æ–‡æ¡£")
            return "æœªèƒ½ä»äº‘ç«¯æ–‡æ¡£ä¸­æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # Step2: é‡æ’
        logging.info("ğŸŒ å¼€å§‹é‡æ’äº‘ç«¯æ–‡æ¡£...")
        reranked_chunks = rerank(context_prompt, retrieved_docs, top_k=3)

        if not reranked_chunks:
            logging.warning("âš ï¸ é‡æ’åæ— ç›¸å…³äº‘ç«¯ç‰‡æ®µ")
            return "æ£€ç´¢åˆ°äº†å†…å®¹ï¼Œä½†æ— æ³•è¯†åˆ«æœ€ç›¸å…³ç‰‡æ®µã€‚"

        # Step3: ç”Ÿæˆç­”æ¡ˆ
        logging.info("ğŸŒ è°ƒç”¨ OpenRouter ç”Ÿæˆç­”æ¡ˆ...")
        api_key = os.getenv("OPENAI_API_KEY") or os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            return "âš ï¸ æœªåœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® OPENAI_API_KEY æˆ– OPENROUTER_API_KEY"

        answer = generate_answer_openai(context_prompt, reranked_chunks, api_key=api_key)
        
        if answer is None:
            return "âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API key æ˜¯å¦æ­£ç¡®æˆ–ç¨åå†è¯•"

        # ç¼“å­˜å¯¹è¯
        conversation_history.append((query, answer))

        logging.info(f"âœ… äº‘ç«¯ç­”æ¡ˆç”Ÿæˆå®Œæˆ: {answer[:100]}...")
        return answer

    except Exception as e:
        logging.error(f"âŒ å¤„ç†äº‘ç«¯é—®é¢˜æ—¶å‡ºé”™: {e}")
        return f"âŒ å¤„ç†äº‘ç«¯é—®é¢˜æ—¶å‡ºé”™: {e}"


if __name__ == "__main__":
    logging.info("ğŸš€ å¯åŠ¨äº‘ç«¯ MCP æœåŠ¡")
    mcp.run(transport="stdio")